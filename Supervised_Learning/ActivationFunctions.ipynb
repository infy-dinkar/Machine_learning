{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82684c5e",
   "metadata": {},
   "source": [
    "# 📘 Beginner's Guide to Activation Functions in Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "## 🌟 What is an Activation Function?\n",
    "An **activation function** acts like a gate in a neural network. It decides:\n",
    "- Should the signal go forward?\n",
    "- How strong should it be?\n",
    "\n",
    "Different functions = different gates.\n",
    "\n",
    "---\n",
    "\n",
    "## 🟡 1. **Sigmoid Function** – The Probability Gate\n",
    "\n",
    "### 🔍 Intuition:\n",
    "Turns any number into a value between **0 and 1**. Useful for binary (yes/no) decisions.\n",
    "\n",
    "> ✅ Large input → output close to 1  \n",
    "> ❌ Negative input → output close to 0  \n",
    "> 🤷 Input near 0 → output around 0.5\n",
    "\n",
    "### 🧮 Formula:\n",
    "```math\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "```\n",
    "\n",
    "### 📈 Behavior:\n",
    "- Input: \\(-\\infty\\) to \\(\\infty\\)\n",
    "- Output: 0 to 1\n",
    "- Shape: \"S\" curve (sigmoid shape)\n",
    "\n",
    "### ✅ Use Case:\n",
    "- Output layer for binary classification\n",
    "\n",
    "### ⚠️ Caution:\n",
    "- Can cause **vanishing gradient** for large inputs\n",
    "- Outputs only **positive values**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔵 2. **Tanh Function** – The Balanced Output Gate\n",
    "\n",
    "### 🔍 Intuition:\n",
    "Similar to Sigmoid, but outputs are between **-1 and 1**. Better for learning.\n",
    "\n",
    "### 🧮 Formula:\n",
    "```math\n",
    "\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
    "```\n",
    "\n",
    "### 📈 Behavior:\n",
    "- Output: -1 to +1\n",
    "- Zero-centered output\n",
    "\n",
    "### ✅ Use Case:\n",
    "- Hidden layers (especially in RNNs)\n",
    "\n",
    "### ⚠️ Caution:\n",
    "- Still suffers from vanishing gradients for large inputs\n",
    "\n",
    "---\n",
    "\n",
    "## 🔴 3. **ReLU (Rectified Linear Unit)** – The Simple Power Switch\n",
    "\n",
    "### 🔍 Intuition:\n",
    "Passes positive values, blocks negatives (outputs 0).\n",
    "\n",
    "### 🧮 Formula:\n",
    "```math\n",
    "f(x) = \\max(0, x)\n",
    "```\n",
    "\n",
    "### 📈 Behavior:\n",
    "- Input: any number\n",
    "- Output: 0 for x<0, else x\n",
    "\n",
    "### ✅ Use Case:\n",
    "- Most common in hidden layers (deep learning)\n",
    "\n",
    "### ⚠️ Caution:\n",
    "- Risk of **dying neurons** (always output 0)\n",
    "\n",
    "---\n",
    "\n",
    "## 🟠 4. **Leaky ReLU** – The Not-Totally-Off Switch\n",
    "\n",
    "### 🔍 Intuition:\n",
    "Fixes dying ReLU by letting a small negative value pass through.\n",
    "\n",
    "### 🧮 Formula:\n",
    "```math\n",
    "f(x) = \\begin{cases} x & \\text{if } x \\geq 0 \\\\ 0.01x & \\text{if } x < 0 \\end{cases}\n",
    "```\n",
    "\n",
    "### ✅ Use Case:\n",
    "- Better stability in training\n",
    "\n",
    "---\n",
    "\n",
    "## 🟢 5. **PReLU (Parametric ReLU)** – The Smart ReLU\n",
    "\n",
    "### 🔍 Intuition:\n",
    "Like Leaky ReLU, but the slope is **learned** during training.\n",
    "\n",
    "### ✅ Use Case:\n",
    "- Advanced models needing flexibility\n",
    "\n",
    "---\n",
    "\n",
    "## 🟣 6. **ELU (Exponential Linear Unit)** – Smooth & Stable Gate\n",
    "\n",
    "### 🔍 Intuition:\n",
    "- Keeps positives like ReLU\n",
    "- Uses a smooth curve for negatives\n",
    "- Zero-centered\n",
    "\n",
    "### 🧮 Formula:\n",
    "```math\n",
    "f(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha(e^x - 1) & \\text{if } x \\leq 0 \\end{cases}\n",
    "```\n",
    "\n",
    "### ✅ Use Case:\n",
    "- Better performance in deep networks\n",
    "\n",
    "---\n",
    "\n",
    "## 🔵 7. **Softmax Function** – The Voting Booth\n",
    "\n",
    "### 🔍 Intuition:\n",
    "Gives probabilities for each class in **multi-class problems**.\n",
    "\n",
    "> Like: \"70% cat 🐱\", \"25% dog 🐶\", \"5% bird 🐦\"\n",
    "\n",
    "### 🧮 Formula:\n",
    "```math\n",
    "\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}\n",
    "```\n",
    "\n",
    "### ✅ Use Case:\n",
    "- Final layer in multi-class classification\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 Summary Table\n",
    "\n",
    "| Function     | Output Range | Good For...               |\n",
    "|--------------|--------------|---------------------------|\n",
    "| Sigmoid      | (0, 1)       | Binary classification     |\n",
    "| Tanh         | (-1, 1)      | Hidden layers, balanced   |\n",
    "| ReLU         | [0, ∞)       | Fast learning             |\n",
    "| Leaky ReLU   | (-∞, ∞)      | Fixing dead neurons       |\n",
    "| PReLU        | (-∞, ∞)      | Learnable slope           |\n",
    "| ELU          | (-1, ∞)      | Smooth + zero-centered    |\n",
    "| Softmax      | (0, 1)       | Multi-class probabilities |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe7ad94",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
