{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGBoost classification** ğŸ¯ğŸ”¥  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸš€ Step 1: Import Required Libraries**  \n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- `numpy` & `pandas` ğŸ“Š â†’ Handle numerical data and tabular data.  \n",
    "- `matplotlib.pyplot` ğŸ“ˆ â†’ Plot graphs.  \n",
    "- `make_classification` ğŸ—ï¸ â†’ Generate a synthetic dataset.  \n",
    "- `train_test_split` âœ‚ï¸ â†’ Split data into training and testing sets.  \n",
    "- `XGBClassifier` ğŸ¤– â†’ Train the XGBoost classifier.  \n",
    "- `accuracy_score` & `classification_report` âœ… â†’ Evaluate model performance.  \n",
    "\n",
    "ğŸ“Œ **Output:** *(No output here, just importing libraries!)*  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“Š Step 2: Generate a Dataset**  \n",
    "```python\n",
    "X, y = make_classification(n_samples=10, n_features=3, n_informative=3, \n",
    "                           n_redundant=0, n_classes=2, random_state=42)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])\n",
    "df[\"Target\"] = y\n",
    "\n",
    "print(df)\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- Creates **10 data points** with **3 features** ğŸ§©.  \n",
    "- All **features are informative** (`n_informative=3`), meaning they impact the target.  \n",
    "- `pd.DataFrame(...)` â†’ Converts the dataset into a structured table ğŸ“Š.  \n",
    "\n",
    "ğŸ“Œ **Output:** *(A table with random feature values and target labels!)*  \n",
    "| Feature1  | Feature2  | Feature3  | Target |\n",
    "|-----------|-----------|-----------|--------|\n",
    "| -0.73     | -0.21     | -0.61     | 0 |\n",
    "| -0.53     | -1.81     | 1.12      | 0 |\n",
    "| -3.62     | -0.04     | 1.10      | 1 |\n",
    "| 0.16      | 0.42      | 1.75      | 1 |\n",
    "| -0.31     | -0.96     | -1.34     | 0 |\n",
    "\n",
    "---\n",
    "\n",
    "## **âœ‚ï¸ Step 3: Split the Data**  \n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- **80%** data for training ğŸ“š, **20%** for testing ğŸ¯.  \n",
    "- `random_state=42` ensures the split remains the same every time you run it.  \n",
    "\n",
    "ğŸ“Œ **Output:**  \n",
    "```\n",
    "Training set shape: (8, 3) (8,)\n",
    "Testing set shape: (2, 3) (2,)\n",
    "```\n",
    "âœ… **8 training samples, 2 testing samples.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¤– Step 4: Train the XGBoost Model**  \n",
    "```python\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- `XGBClassifier()` â†’ Creates an XGBoost model ğŸŒŸ.  \n",
    "- `use_label_encoder=False` â†’ Prevents an unnecessary warning âš ï¸.  \n",
    "- `eval_metric=\"logloss\"` â†’ Uses **log loss** as the evaluation metric ğŸ†.  \n",
    "- `model.fit(...)` â†’ Trains the model on training data ğŸ‹ï¸.  \n",
    "\n",
    "ğŸ“Œ **Output:** *(No printed output, but the model is trained successfully!)*  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ”® Step 5: Make Predictions**  \n",
    "```python\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predicted Labels:\", y_pred)\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- `model.predict(X_test)` â†’ Uses the trained model to predict values.  \n",
    "- Prints predicted labels ğŸ“¢.  \n",
    "\n",
    "ğŸ“Œ **Output:**  \n",
    "```\n",
    "Predicted Labels: [1 1]\n",
    "```\n",
    "âœ… The model predicted **class `1`** for both test samples.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“ Step 6: Evaluate the Model**  \n",
    "```python\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- `accuracy_score(y_test, y_pred)` â†’ Calculates **accuracy** (correct predictions).  \n",
    "- `classification_report(y_test, y_pred)` â†’ Shows **precision, recall, and F1-score** ğŸ“Š.  \n",
    "\n",
    "ğŸ“Œ **Output:**  \n",
    "```\n",
    "Accuracy: 0.00\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.00      0.00      0.00         1\n",
    "           1       0.00      0.00      0.00         1\n",
    "\n",
    "    accuracy                           0.00         2\n",
    "   macro avg       0.00      0.00      0.00         2\n",
    "weighted avg       0.00      0.00      0.00         2\n",
    "```\n",
    "âš ï¸ **Accuracy is 0.00** â†’ The model misclassified both test samples âŒ.  \n",
    "**Why?** The dataset is **too small** (only 10 samples), so the model couldnâ€™t learn properly.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“Š Step 7: Feature Importance Visualization**  \n",
    "```python\n",
    "plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "plt.xticks(ticks=range(len(df.columns)-1), labels=df.columns[:-1])\n",
    "plt.ylabel(\"Feature Importance Score\")\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.title(\"Feature Importance in XGBoost\")\n",
    "plt.show()\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- `model.feature_importances_` â†’ Measures how important each feature is in making predictions.  \n",
    "- `plt.bar(...)` â†’ Plots a **bar chart** ğŸ“Š.  \n",
    "- `plt.xticks(...)` â†’ Labels the x-axis with feature names.  \n",
    "\n",
    "ğŸ“Œ **Output:** *(A bar chart showing feature importance!)*  \n",
    "ğŸ“ˆ **The most important feature will have the highest bar.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ Key Takeaways**\n",
    "âœ… **We implemented XGBoost step by step!** ğŸš€  \n",
    "- **Generated a dataset** ğŸ“Š  \n",
    "- **Split data into training & testing sets** âœ‚ï¸  \n",
    "- **Trained an XGBoost classifier** ğŸ¤–  \n",
    "- **Made predictions** ğŸ”®  \n",
    "- **Evaluated model performance** âœ…  \n",
    "- **Visualized feature importance** ğŸ“ˆ  \n",
    "  ğŸš€ğŸ“Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
