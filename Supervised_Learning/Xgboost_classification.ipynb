{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGBoost classification** 🎯🔥  \n",
    "\n",
    "---\n",
    "\n",
    "## **🚀 Step 1: Import Required Libraries**  \n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- `numpy` & `pandas` 📊 → Handle numerical data and tabular data.  \n",
    "- `matplotlib.pyplot` 📈 → Plot graphs.  \n",
    "- `make_classification` 🏗️ → Generate a synthetic dataset.  \n",
    "- `train_test_split` ✂️ → Split data into training and testing sets.  \n",
    "- `XGBClassifier` 🤖 → Train the XGBoost classifier.  \n",
    "- `accuracy_score` & `classification_report` ✅ → Evaluate model performance.  \n",
    "\n",
    "📌 **Output:** *(No output here, just importing libraries!)*  \n",
    "\n",
    "---\n",
    "\n",
    "## **📊 Step 2: Generate a Dataset**  \n",
    "```python\n",
    "X, y = make_classification(n_samples=10, n_features=3, n_informative=3, \n",
    "                           n_redundant=0, n_classes=2, random_state=42)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])\n",
    "df[\"Target\"] = y\n",
    "\n",
    "print(df)\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- Creates **10 data points** with **3 features** 🧩.  \n",
    "- All **features are informative** (`n_informative=3`), meaning they impact the target.  \n",
    "- `pd.DataFrame(...)` → Converts the dataset into a structured table 📊.  \n",
    "\n",
    "📌 **Output:** *(A table with random feature values and target labels!)*  \n",
    "| Feature1  | Feature2  | Feature3  | Target |\n",
    "|-----------|-----------|-----------|--------|\n",
    "| -0.73     | -0.21     | -0.61     | 0 |\n",
    "| -0.53     | -1.81     | 1.12      | 0 |\n",
    "| -3.62     | -0.04     | 1.10      | 1 |\n",
    "| 0.16      | 0.42      | 1.75      | 1 |\n",
    "| -0.31     | -0.96     | -1.34     | 0 |\n",
    "\n",
    "---\n",
    "\n",
    "## **✂️ Step 3: Split the Data**  \n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- **80%** data for training 📚, **20%** for testing 🎯.  \n",
    "- `random_state=42` ensures the split remains the same every time you run it.  \n",
    "\n",
    "📌 **Output:**  \n",
    "```\n",
    "Training set shape: (8, 3) (8,)\n",
    "Testing set shape: (2, 3) (2,)\n",
    "```\n",
    "✅ **8 training samples, 2 testing samples.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **🤖 Step 4: Train the XGBoost Model**  \n",
    "```python\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- `XGBClassifier()` → Creates an XGBoost model 🌟.  \n",
    "- `use_label_encoder=False` → Prevents an unnecessary warning ⚠️.  \n",
    "- `eval_metric=\"logloss\"` → Uses **log loss** as the evaluation metric 🏆.  \n",
    "- `model.fit(...)` → Trains the model on training data 🏋️.  \n",
    "\n",
    "📌 **Output:** *(No printed output, but the model is trained successfully!)*  \n",
    "\n",
    "---\n",
    "\n",
    "## **🔮 Step 5: Make Predictions**  \n",
    "```python\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predicted Labels:\", y_pred)\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- `model.predict(X_test)` → Uses the trained model to predict values.  \n",
    "- Prints predicted labels 📢.  \n",
    "\n",
    "📌 **Output:**  \n",
    "```\n",
    "Predicted Labels: [1 1]\n",
    "```\n",
    "✅ The model predicted **class `1`** for both test samples.  \n",
    "\n",
    "---\n",
    "\n",
    "## **📏 Step 6: Evaluate the Model**  \n",
    "```python\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- `accuracy_score(y_test, y_pred)` → Calculates **accuracy** (correct predictions).  \n",
    "- `classification_report(y_test, y_pred)` → Shows **precision, recall, and F1-score** 📊.  \n",
    "\n",
    "📌 **Output:**  \n",
    "```\n",
    "Accuracy: 0.00\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.00      0.00      0.00         1\n",
    "           1       0.00      0.00      0.00         1\n",
    "\n",
    "    accuracy                           0.00         2\n",
    "   macro avg       0.00      0.00      0.00         2\n",
    "weighted avg       0.00      0.00      0.00         2\n",
    "```\n",
    "⚠️ **Accuracy is 0.00** → The model misclassified both test samples ❌.  \n",
    "**Why?** The dataset is **too small** (only 10 samples), so the model couldn’t learn properly.  \n",
    "\n",
    "---\n",
    "\n",
    "## **📊 Step 7: Feature Importance Visualization**  \n",
    "```python\n",
    "plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "plt.xticks(ticks=range(len(df.columns)-1), labels=df.columns[:-1])\n",
    "plt.ylabel(\"Feature Importance Score\")\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.title(\"Feature Importance in XGBoost\")\n",
    "plt.show()\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- `model.feature_importances_` → Measures how important each feature is in making predictions.  \n",
    "- `plt.bar(...)` → Plots a **bar chart** 📊.  \n",
    "- `plt.xticks(...)` → Labels the x-axis with feature names.  \n",
    "\n",
    "📌 **Output:** *(A bar chart showing feature importance!)*  \n",
    "📈 **The most important feature will have the highest bar.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Key Takeaways**\n",
    "✅ **We implemented XGBoost step by step!** 🚀  \n",
    "- **Generated a dataset** 📊  \n",
    "- **Split data into training & testing sets** ✂️  \n",
    "- **Trained an XGBoost classifier** 🤖  \n",
    "- **Made predictions** 🔮  \n",
    "- **Evaluated model performance** ✅  \n",
    "- **Visualized feature importance** 📈  \n",
    "  🚀📊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
