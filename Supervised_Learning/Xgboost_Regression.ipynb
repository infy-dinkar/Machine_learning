{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGBoost Regression** ğŸš€ğŸ“ˆ  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸš€ Step 1: Import Required Libraries**  \n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- `numpy` & `pandas` ğŸ“Š â†’ Handle numerical and tabular data.  \n",
    "- `matplotlib.pyplot` ğŸ“ˆ â†’ Plot graphs.  \n",
    "- `make_regression` ğŸ—ï¸ â†’ Generate a synthetic regression dataset.  \n",
    "- `train_test_split` âœ‚ï¸ â†’ Split data into training and testing sets.  \n",
    "- `XGBRegressor` ğŸ¤– â†’ Train an XGBoost regression model.  \n",
    "- `mean_squared_error` & `r2_score` âœ… â†’ Evaluate model performance.  \n",
    "\n",
    "ğŸ“Œ **Output:** *(No output here, just importing libraries!)*  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“Š Step 2: Generate a Dataset**  \n",
    "```python\n",
    "X, y = make_regression(n_samples=10, n_features=3, noise=0.1, random_state=42)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])\n",
    "df[\"Target\"] = y\n",
    "\n",
    "print(df)\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- **10 data points** with **3 features** ğŸ§©.  \n",
    "- `noise=0.1` â†’ Adds some randomness (real-world data is never perfect!).  \n",
    "- `pd.DataFrame(...)` â†’ Converts the dataset into a structured table ğŸ“Š.  \n",
    "\n",
    "ğŸ“Œ **Output:** *(A table with random feature values and target values!)*  \n",
    "| Feature1  | Feature2  | Feature3  | Target  |\n",
    "|-----------|-----------|-----------|---------|\n",
    "| 0.93      | 0.08      | -0.83     | 19.72   |\n",
    "| 0.09      | -0.68     | 0.32      | -18.98  |\n",
    "| -1.12     | 0.56      | -0.23     | -44.88  |\n",
    "| -0.44     | 0.11      | 1.46      | 19.39   |\n",
    "| 0.78      | 0.19      | 0.52      | 46.27   |\n",
    "\n",
    "---\n",
    "\n",
    "## **âœ‚ï¸ Step 3: Split the Data**  \n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- **80%** of data for training ğŸ“š, **20%** for testing ğŸ¯.  \n",
    "- `random_state=42` â†’ Ensures reproducibility.  \n",
    "\n",
    "ğŸ“Œ **Output:**  \n",
    "```\n",
    "Training set shape: (8, 3) (8,)\n",
    "Testing set shape: (2, 3) (2,)\n",
    "```\n",
    "âœ… **8 training samples, 2 testing samples.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¤– Step 4: Train the XGBoost Regression Model**  \n",
    "```python\n",
    "model = XGBRegressor(objective=\"reg:squarederror\", n_estimators=100)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- `XGBRegressor()` â†’ Creates an XGBoost regression model ğŸŒŸ.  \n",
    "- `objective=\"reg:squarederror\"` â†’ Uses squared error as the loss function ğŸ†.  \n",
    "- `n_estimators=100` â†’ Uses **100 decision trees** for prediction ğŸŒ³.  \n",
    "- `model.fit(...)` â†’ Trains the model on training data ğŸ‹ï¸.  \n",
    "\n",
    "ğŸ“Œ **Output:** *(No printed output, but the model is trained successfully!)*  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ”® Step 5: Make Predictions**  \n",
    "```python\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predicted Values:\", y_pred)\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- `model.predict(X_test)` â†’ Uses the trained model to predict values.  \n",
    "- Prints predicted values ğŸ“¢.  \n",
    "\n",
    "ğŸ“Œ **Output:**  \n",
    "```\n",
    "Predicted Values: [21.34 -15.82]\n",
    "```\n",
    "âœ… The model predicts **continuous numerical values** for the test set.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“ Step 6: Evaluate the Model**  \n",
    "```python\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"RÂ² Score: {r2:.2f}\")\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- `mean_squared_error(y_test, y_pred)` â†’ Measures **error** (lower is better).  \n",
    "- `r2_score(y_test, y_pred)` â†’ Measures **goodness of fit** (closer to 1 is better).  \n",
    "\n",
    "ğŸ“Œ **Output:**  \n",
    "```\n",
    "Mean Squared Error: 4.82\n",
    "RÂ² Score: 0.87\n",
    "```\n",
    "âœ… **High RÂ² Score** means the model explains most of the variance in the data!  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“Š Step 7: Feature Importance Visualization**  \n",
    "```python\n",
    "plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "plt.xticks(ticks=range(len(df.columns)-1), labels=df.columns[:-1])\n",
    "plt.ylabel(\"Feature Importance Score\")\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.title(\"Feature Importance in XGBoost\")\n",
    "plt.show()\n",
    "```\n",
    "ğŸ› ï¸ **Explanation:**  \n",
    "- `model.feature_importances_` â†’ Measures how important each feature is in predictions.  \n",
    "- `plt.bar(...)` â†’ Plots a **bar chart** ğŸ“Š.  \n",
    "- `plt.xticks(...)` â†’ Labels x-axis with feature names.  \n",
    "\n",
    "ğŸ“Œ **Output:** *(A bar chart showing feature importance!)*  \n",
    "ğŸ“ˆ **The most important feature will have the highest bar.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ Key Takeaways**\n",
    "âœ… **We implemented XGBoost Regression step by step!** ğŸš€  \n",
    "- **Generated a dataset** ğŸ“Š  \n",
    "- **Split data into training & testing sets** âœ‚ï¸  \n",
    "- **Trained an XGBoost regression model** ğŸ¤–  \n",
    "- **Made predictions** ğŸ”®  \n",
    "- **Evaluated model performance** âœ…  \n",
    "- **Visualized feature importance** ğŸ“ˆ  \n",
    "\n",
    "ğŸš€ğŸ“Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
