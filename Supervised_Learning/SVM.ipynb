{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎉🚀  \n",
    "\n",
    "---\n",
    "\n",
    "# 🎯 **Understanding Support Vector Machines (SVMs) in Detail!**  \n",
    "\n",
    "## **📌 Step 1: Importing Required Libraries**  \n",
    "The **first step** in any **machine learning** project is to **import the required libraries**. These libraries provide us with the necessary functions for **data processing, visualization, model training, and evaluation**.  \n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "```\n",
    "\n",
    "### 🔍 **Breaking Down the Imports:**\n",
    "1️⃣ `import numpy as np` ➡ **NumPy** helps with numerical computations, arrays, and mathematical operations. 🧮📊  \n",
    "\n",
    "2️⃣ `import matplotlib.pyplot as plt` ➡ **Matplotlib** is a visualization library that helps create graphs, scatter plots, and decision boundaries. 📈🎨  \n",
    "\n",
    "3️⃣ `from sklearn.svm import SVC, SVR`  \n",
    "   - **SVC (Support Vector Classifier)** ➡ Used for classification tasks (predicting categories like spam vs non-spam). 📩✅  \n",
    "   - **SVR (Support Vector Regression)** ➡ Used for regression tasks (predicting continuous values like temperature). 🌡📉  \n",
    "\n",
    "4️⃣ `from sklearn.datasets import make_classification, make_regression`  \n",
    "   - `make_classification()` ➡ Generates synthetic **classification** data for models. 🎭📊  \n",
    "   - `make_regression()` ➡ Generates synthetic **regression** data for predicting numerical values.  \n",
    "\n",
    "5️⃣ `from sklearn.model_selection import train_test_split`  \n",
    "   - Splits the dataset into **training** and **testing** sets. Training is for model learning, and testing is for evaluation. 🎓🔬  \n",
    "\n",
    "6️⃣ `from sklearn.preprocessing import StandardScaler`  \n",
    "   - Standardizes data so that all features have a **mean of 0** and **standard deviation of 1**. This improves SVM performance. 📏⚖  \n",
    "\n",
    "7️⃣ `from sklearn.metrics import accuracy_score, mean_squared_error`  \n",
    "   - **`accuracy_score()`** ➡ Measures how **many predictions** were correct for classification. 🎯✅  \n",
    "   - **`mean_squared_error()`** ➡ Measures how **far the predictions are** from actual values in regression. 📏📉  \n",
    "\n",
    "8️⃣ `from sklearn.pipeline import make_pipeline`  \n",
    "   - A **pipeline** helps in chaining multiple steps together, like **scaling data** before passing it to the SVM model. 🔗🛠  \n",
    "\n",
    "9️⃣ `from sklearn.svm import LinearSVC`  \n",
    "   - **LinearSVC** is an optimized version of **SVC** when using a **linear kernel**. It is **faster and efficient** for large datasets. 🚀⚡  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Step 2: Support Vector Classifier (SVC)**\n",
    "Now, let's **generate a classification dataset** and train an **SVM model** to **classify data into two categories**. 🏷🎭  \n",
    "\n",
    "```python\n",
    "print(\"\\n--- Support Vector Classifier (SVC) ---\\n\")\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_classes=2, random_state=42)\n",
    "```\n",
    "- **`make_classification(n_samples=500, n_features=2, n_classes=2, random_state=42)`**  \n",
    "  - Generates **500 samples**, with **2 features per sample**.  \n",
    "  - The data is **divided into 2 classes** (Binary Classification).  \n",
    "  - `random_state=42` ensures the **same dataset** is created every time we run the code. 🎲🔄  \n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "- **Splitting Data into Training & Testing Sets**  \n",
    "  - `X_train`, `y_train` ➡ **Used to train the model** 🏋️‍♂️📚  \n",
    "  - `X_test`, `y_test` ➡ **Used to evaluate the model** 🧪🔬  \n",
    "  - `test_size=0.2` ➡ **20% of data is reserved for testing.**  \n",
    "\n",
    "```python\n",
    "svc = make_pipeline(StandardScaler(), SVC(kernel='linear', C=1.0))\n",
    "```\n",
    "- **Creating the SVM Model** 🎯  \n",
    "  - **`StandardScaler()`** ➡ **Normalizes the data** before passing it to the model. 📏📊  \n",
    "  - **`SVC(kernel='linear', C=1.0)`** ➡ Uses a **linear kernel** for classification.  \n",
    "  - **C=1.0** ➡ Controls **how much misclassified points** are penalized. Higher `C` = fewer misclassifications. ✅  \n",
    "\n",
    "```python\n",
    "svc.fit(X_train, y_train)\n",
    "```\n",
    "- **Training the SVM classifier** using the **training data**. 🏋️‍♂️🎓  \n",
    "\n",
    "```python\n",
    "y_pred = svc.predict(X_test)\n",
    "```\n",
    "- **Making Predictions** on unseen **test data**. 🤖📊  \n",
    "\n",
    "```python\n",
    "print(\"SVC Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "- **Evaluating Model Performance**  \n",
    "  - Compares predicted labels (`y_pred`) with actual labels (`y_test`). ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Step 3: Trying Different SVM Kernels**\n",
    "SVM can use **different kernels** to handle **complex patterns**. Let's test them all! 🎭🔬  \n",
    "\n",
    "```python\n",
    "print(\"\\n--- SVM Kernels Implementation ---\\n\")\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "```\n",
    "- We define a list of **kernel functions** to try. 🔍📚  \n",
    "\n",
    "```python\n",
    "for kernel in kernels:\n",
    "    model = make_pipeline(StandardScaler(), SVC(kernel=kernel, C=1.0, degree=3 if kernel == 'poly' else None))\n",
    "```\n",
    "- **Iterating Through Different Kernels** 🔄  \n",
    "  - `linear` ➡ Best for **simple, linearly separable data**. 📏✅  \n",
    "  - `poly` ➡ Uses **polynomial transformation** (good for curved boundaries). 🔄✨  \n",
    "  - `rbf` ➡ **Radial Basis Function Kernel**, great for **complex non-linear data**. 🔥🌀  \n",
    "  - `sigmoid` ➡ Similar to **neural networks**, maps data to a **higher dimension**. 🤖🧠  \n",
    "\n",
    "```python\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"{kernel} Kernel Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "```\n",
    "- **Train & Test Each Kernel** 🏋️‍♂️🔬  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Step 4: Visualizing Decision Boundaries**\n",
    "SVM decision boundaries help understand **how well the model is classifying the data**. 📊🎨  \n",
    "\n",
    "```python\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axes = axes.ravel()\n",
    "```\n",
    "- **Creating a 2x2 grid of subplots** for visualization. 📊🔍  \n",
    "\n",
    "```python\n",
    "for i, kernel in enumerate(kernels):\n",
    "    model = make_pipeline(StandardScaler(), SVC(kernel=kernel, C=1.0))\n",
    "    model.fit(X_train, y_train)\n",
    "```\n",
    "- **Training a Model for Each Kernel** 🚀  \n",
    "\n",
    "```python\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "```\n",
    "- **Generating Grid Points for Decision Boundary** 📏📊  \n",
    "\n",
    "```python\n",
    "Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "```\n",
    "- **Predicting Labels for Grid Points** 🎯✅  \n",
    "\n",
    "```python\n",
    "axes[i].contourf(xx, yy, Z, alpha=0.3)\n",
    "axes[i].scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')\n",
    "axes[i].set_title(f\"SVM with {kernel} kernel\")\n",
    "```\n",
    "- **Plotting Decision Boundaries for Each Kernel** 🎨📊  \n",
    "\n",
    "---\n",
    "\n",
    " 🚀🔥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.datasets import make_classification, make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# --- Step 2: Support Vector Classifier (SVC) ---\n",
    "print(\"\\n--- Support Vector Classifier (SVC) ---\\n\")\n",
    "X, y = make_classification(n_samples=500, n_features=2, n_classes=2, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svc = make_pipeline(StandardScaler(), SVC(kernel='linear', C=1.0))\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "print(\"SVC Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# --- Step 3: Trying Different SVM Kernels ---\n",
    "print(\"\\n--- SVM Kernels Implementation ---\\n\")\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "for kernel in kernels:\n",
    "    model = make_pipeline(StandardScaler(), SVC(kernel=kernel, C=1.0, degree=3 if kernel == 'poly' else None))\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(f\"{kernel} Kernel Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# --- Step 4: Visualizing Decision Boundaries ---\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, kernel in enumerate(kernels):\n",
    "    model = make_pipeline(StandardScaler(), SVC(kernel=kernel, C=1.0))\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    axes[i].contourf(xx, yy, Z, alpha=0.3)\n",
    "    axes[i].scatter(X[:, 0], X[:, 1], c=y, edgecolor='k')\n",
    "    axes[i].set_title(f\"SVM with {kernel} kernel\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
