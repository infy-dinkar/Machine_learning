{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go step by step  ğŸš€  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸ”µ Step 1: Import Required Libraries**\n",
    "First, we need to **import** all necessary libraries that help in **data handling, machine learning, and evaluation**.\n",
    "\n",
    "```python\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For handling datasets\n",
    "\n",
    "# Splitting dataset into training & testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing different NaÃ¯ve Bayes models\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB, ComplementNB  \n",
    "\n",
    "# Preprocessing utilities\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  \n",
    "\n",
    "# Model evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, log_loss, roc_auc_score\n",
    "```\n",
    "\n",
    "### **ğŸ“Œ Why these libraries?**\n",
    "- `numpy` â†’ Helps in numerical computations.  \n",
    "- `pandas` â†’ Helps in handling tabular data.  \n",
    "- `train_test_split` â†’ Splits dataset into **training** and **testing** parts.  \n",
    "- `sklearn.naive_bayes` â†’ Provides **4 different NaÃ¯ve Bayes models**.  \n",
    "- `StandardScaler` & `LabelEncoder` â†’ Used for **data preprocessing**.  \n",
    "- `TfidfVectorizer` â†’ Converts **text into numerical data**.  \n",
    "- `metrics` â†’ Evaluates the **performance of models**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸŸ¢ Step 2: Generate a Dummy Dataset**\n",
    "Now, letâ€™s **create a fake dataset** with a mix of **numerical, categorical, and text features**.\n",
    "\n",
    "```python\n",
    "# Setting seed for reproducibility\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Creating the dataset\n",
    "data = pd.DataFrame({\n",
    "    'Age': np.random.randint(18, 60, 100),  # Random ages between 18-60\n",
    "    'Salary': np.random.randint(20000, 100000, 100),  # Random salaries\n",
    "    'Job Type': np.random.choice(['Engineer', 'Doctor', 'Teacher', 'Lawyer'], 100),  # Categorical column\n",
    "    'City': np.random.choice(['New York', 'San Francisco', 'Chicago', 'Los Angeles'], 100),  # Another categorical\n",
    "    'Review': np.random.choice(['Great service!', 'Terrible experience.', 'Okay, but not great.', 'Loved it!'], 100),  # Text column\n",
    "    'Purchased': np.random.choice([0, 1], 100)  # Target (0 = No, 1 = Yes)\n",
    "})\n",
    "\n",
    "# Display first 5 rows of dataset\n",
    "print(data.head())\n",
    "```\n",
    "\n",
    "### **ğŸ“Œ Understanding the dataset:**\n",
    "| Age | Salary | Job Type | City | Review | Purchased |\n",
    "|----|--------|---------|------|--------|-----------|\n",
    "| 35 | 85000  | Doctor  | NYC  | Great service!  | 1 |\n",
    "| 22 | 50000  | Lawyer  | LA   | Terrible experience. | 0 |\n",
    "| 48 | 70000  | Engineer| SF   | Loved it!  | 1 |\n",
    "| 29 | 65000  | Teacher | Chicago | Okay, but not great. | 0 |\n",
    "\n",
    "- **Age & Salary** â†’ **Numerical features** (âœ… Good for GaussianNB).  \n",
    "- **Job Type & City** â†’ **Categorical features** (ğŸ”„ Need encoding).  \n",
    "- **Review** â†’ **Text feature** (ğŸ”„ Needs vectorization).  \n",
    "- **Purchased** â†’ **Target column** (ğŸš€ The variable we want to predict).  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸŸ  Step 3: Data Preprocessing**\n",
    "### **ğŸ”¹ 1. Encoding Categorical Features (Job Type, City)**\n",
    "Since NaÃ¯ve Bayes models only work with **numbers**, we need to **convert categorical features into numbers**.\n",
    "\n",
    "```python\n",
    "# Initialize Label Encoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Encode categorical features\n",
    "data['Job Type'] = le.fit_transform(data['Job Type'])\n",
    "data['City'] = le.fit_transform(data['City'])\n",
    "\n",
    "# Display updated dataset\n",
    "print(data.head())\n",
    "```\n",
    "\n",
    "### **ğŸ“Œ What happened here?**\n",
    "- `Engineer`, `Doctor`, `Lawyer`, etc., are **converted into numbers** (`0,1,2,3`).  \n",
    "- `New York`, `Chicago`, etc., are also **converted into numbers**.  \n",
    "\n",
    "âœ… Now, all categorical columns are **numerical**, and the dataset is ready for training!  \n",
    "\n",
    "---\n",
    "\n",
    "### **ğŸ”¹ 2. Convert Text Data into Numerical Features using TF-IDF**\n",
    "For text features like `Review`, we use **TF-IDF Vectorization** to convert words into **numerical values**.\n",
    "\n",
    "```python\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform text data into numerical vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(data['Review']).toarray()\n",
    "\n",
    "# Convert TF-IDF array to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Drop original text column and merge TF-IDF data\n",
    "data = data.drop(columns=['Review']).reset_index(drop=True)\n",
    "data = pd.concat([data, tfidf_df], axis=1)\n",
    "\n",
    "# Display dataset after text processing\n",
    "print(data.head())\n",
    "```\n",
    "\n",
    "âœ… Now, the `Review` column is **replaced with TF-IDF features**, making the dataset fully numerical! ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸ”µ Step 4: Splitting the Dataset**\n",
    "We split our dataset into **80% training** and **20% testing**.\n",
    "\n",
    "```python\n",
    "X = data.drop(columns=['Purchased'])  # Features\n",
    "y = data['Purchased']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "ğŸ“Œ **Why split the dataset?**  \n",
    "- **Training data** â†’ Used to teach the model.  \n",
    "- **Testing data** â†’ Used to check the modelâ€™s accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸŸ¢ Step 5: Apply Different NaÃ¯ve Bayes Models**\n",
    "### **1ï¸âƒ£ Gaussian NaÃ¯ve Bayes (For Continuous Data)**\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "y_pred_gnb = gnb.predict(X_test_scaled)\n",
    "\n",
    "print(\"GaussianNB Accuracy:\", accuracy_score(y_test, y_pred_gnb))\n",
    "```\n",
    "âœ” Used for **continuous numerical data**.  \n",
    "âœ” **Standardization improves performance**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2ï¸âƒ£ Multinomial NaÃ¯ve Bayes (For Text Data)**\n",
    "```python\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = mnb.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB Accuracy:\", accuracy_score(y_test, y_pred_mnb))\n",
    "```\n",
    "âœ” Used for **text-based classification** (e.g., spam detection).  \n",
    "\n",
    "---\n",
    "\n",
    "### **3ï¸âƒ£ Bernoulli NaÃ¯ve Bayes (For Binary Data)**\n",
    "```python\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred_bnb = bnb.predict(X_test)\n",
    "\n",
    "print(\"BernoulliNB Accuracy:\", accuracy_score(y_test, y_pred_bnb))\n",
    "```\n",
    "âœ” Used for **binary feature data** (e.g., word presence/absence).  \n",
    "\n",
    "---\n",
    "\n",
    "### **4ï¸âƒ£ Complement NaÃ¯ve Bayes (For Imbalanced Data)**\n",
    "```python\n",
    "cnb = ComplementNB()\n",
    "cnb.fit(X_train, y_train)\n",
    "y_pred_cnb = cnb.predict(X_test)\n",
    "\n",
    "print(\"ComplementNB Accuracy:\", accuracy_score(y_test, y_pred_cnb))\n",
    "```\n",
    "âœ” Best for **imbalanced datasets**.  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸŸ  Step 6: Model Evaluation**\n",
    "```python\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_gnb))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gnb))\n",
    "print(\"Log Loss:\", log_loss(y_test, gnb.predict_proba(X_test_scaled)))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, gnb.predict_proba(X_test_scaled)[:, 1]))\n",
    "```\n",
    "\n",
    "âœ… **Accuracy** â†’ Measures correct predictions.  \n",
    "âœ… **Log Loss** â†’ Lower is better.  \n",
    "âœ… **ROC-AUC** â†’ Measures class separation.  \n",
    "\n",
    "---\n",
    "\n",
    "### **ğŸ¯ Summary**\n",
    "ğŸ”¥ We applied **all NaÃ¯ve Bayes models** step by step!  \n",
    "ğŸ”¥ Converted **categorical & text data** into numerical form!  \n",
    "ğŸ”¥ Evaluated models using **accuracy, log loss, and ROC-AUC**!  \n",
    "\n",
    " ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
