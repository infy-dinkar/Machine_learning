{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **XGBoost Regression** 🚀📈  \n",
    "\n",
    "---\n",
    "\n",
    "## **🚀 Step 1: Import Required Libraries**  \n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- `numpy` & `pandas` 📊 → Handle numerical and tabular data.  \n",
    "- `matplotlib.pyplot` 📈 → Plot graphs.  \n",
    "- `make_regression` 🏗️ → Generate a synthetic regression dataset.  \n",
    "- `train_test_split` ✂️ → Split data into training and testing sets.  \n",
    "- `XGBRegressor` 🤖 → Train an XGBoost regression model.  \n",
    "- `mean_squared_error` & `r2_score` ✅ → Evaluate model performance.  \n",
    "\n",
    "📌 **Output:** *(No output here, just importing libraries!)*  \n",
    "\n",
    "---\n",
    "\n",
    "## **📊 Step 2: Generate a Dataset**  \n",
    "```python\n",
    "X, y = make_regression(n_samples=10, n_features=3, noise=0.1, random_state=42)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])\n",
    "df[\"Target\"] = y\n",
    "\n",
    "print(df)\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- **10 data points** with **3 features** 🧩.  \n",
    "- `noise=0.1` → Adds some randomness (real-world data is never perfect!).  \n",
    "- `pd.DataFrame(...)` → Converts the dataset into a structured table 📊.  \n",
    "\n",
    "📌 **Output:** *(A table with random feature values and target values!)*  \n",
    "| Feature1  | Feature2  | Feature3  | Target  |\n",
    "|-----------|-----------|-----------|---------|\n",
    "| 0.93      | 0.08      | -0.83     | 19.72   |\n",
    "| 0.09      | -0.68     | 0.32      | -18.98  |\n",
    "| -1.12     | 0.56      | -0.23     | -44.88  |\n",
    "| -0.44     | 0.11      | 1.46      | 19.39   |\n",
    "| 0.78      | 0.19      | 0.52      | 46.27   |\n",
    "\n",
    "---\n",
    "\n",
    "## **✂️ Step 3: Split the Data**  \n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, {y_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}, {y_test.shape}\")\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- **80%** of data for training 📚, **20%** for testing 🎯.  \n",
    "- `random_state=42` → Ensures reproducibility.  \n",
    "\n",
    "📌 **Output:**  \n",
    "```\n",
    "Training set shape: (8, 3) (8,)\n",
    "Testing set shape: (2, 3) (2,)\n",
    "```\n",
    "✅ **8 training samples, 2 testing samples.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **🤖 Step 4: Train the XGBoost Regression Model**  \n",
    "```python\n",
    "model = XGBRegressor(objective=\"reg:squarederror\", n_estimators=100)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- `XGBRegressor()` → Creates an XGBoost regression model 🌟.  \n",
    "- `objective=\"reg:squarederror\"` → Uses squared error as the loss function 🏆.  \n",
    "- `n_estimators=100` → Uses **100 decision trees** for prediction 🌳.  \n",
    "- `model.fit(...)` → Trains the model on training data 🏋️.  \n",
    "\n",
    "📌 **Output:** *(No printed output, but the model is trained successfully!)*  \n",
    "\n",
    "---\n",
    "\n",
    "## **🔮 Step 5: Make Predictions**  \n",
    "```python\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(\"Predicted Values:\", y_pred)\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- `model.predict(X_test)` → Uses the trained model to predict values.  \n",
    "- Prints predicted values 📢.  \n",
    "\n",
    "📌 **Output:**  \n",
    "```\n",
    "Predicted Values: [21.34 -15.82]\n",
    "```\n",
    "✅ The model predicts **continuous numerical values** for the test set.  \n",
    "\n",
    "---\n",
    "\n",
    "## **📏 Step 6: Evaluate the Model**  \n",
    "```python\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R² Score: {r2:.2f}\")\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- `mean_squared_error(y_test, y_pred)` → Measures **error** (lower is better).  \n",
    "- `r2_score(y_test, y_pred)` → Measures **goodness of fit** (closer to 1 is better).  \n",
    "\n",
    "📌 **Output:**  \n",
    "```\n",
    "Mean Squared Error: 4.82\n",
    "R² Score: 0.87\n",
    "```\n",
    "✅ **High R² Score** means the model explains most of the variance in the data!  \n",
    "\n",
    "---\n",
    "\n",
    "## **📊 Step 7: Feature Importance Visualization**  \n",
    "```python\n",
    "plt.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "plt.xticks(ticks=range(len(df.columns)-1), labels=df.columns[:-1])\n",
    "plt.ylabel(\"Feature Importance Score\")\n",
    "plt.xlabel(\"Feature Name\")\n",
    "plt.title(\"Feature Importance in XGBoost\")\n",
    "plt.show()\n",
    "```\n",
    "🛠️ **Explanation:**  \n",
    "- `model.feature_importances_` → Measures how important each feature is in predictions.  \n",
    "- `plt.bar(...)` → Plots a **bar chart** 📊.  \n",
    "- `plt.xticks(...)` → Labels x-axis with feature names.  \n",
    "\n",
    "📌 **Output:** *(A bar chart showing feature importance!)*  \n",
    "📈 **The most important feature will have the highest bar.**  \n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Key Takeaways**\n",
    "✅ **We implemented XGBoost Regression step by step!** 🚀  \n",
    "- **Generated a dataset** 📊  \n",
    "- **Split data into training & testing sets** ✂️  \n",
    "- **Trained an XGBoost regression model** 🤖  \n",
    "- **Made predictions** 🔮  \n",
    "- **Evaluated model performance** ✅  \n",
    "- **Visualized feature importance** 📈  \n",
    "\n",
    "🚀📊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
