{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Letâ€™s focus only on **Bagging** . ğŸ§‘â€ğŸ’»âœ¨  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸš€ Step 1: Import Required Libraries**\n",
    "```python\n",
    "import numpy as np  # ğŸ“Š For numerical operations\n",
    "import pandas as pd  # ğŸ“ For data handling\n",
    "from sklearn.model_selection import train_test_split  # ğŸ”€ For splitting data\n",
    "from sklearn.ensemble import BaggingClassifier  # ğŸ­ Bagging model\n",
    "from sklearn.tree import DecisionTreeClassifier  # ğŸŒ³ Base model (Decision Tree)\n",
    "from sklearn.metrics import accuracy_score  # âœ… For evaluating model performance\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `numpy` (`np`) helps us generate random numbers for our dataset. ğŸ“Š  \n",
    "- `pandas` (`pd`) allows us to structure the dataset into a table. ğŸ“  \n",
    "- `train_test_split` will **divide** our data into **training** and **testing** sets. ğŸ”€  \n",
    "- `BaggingClassifier` is the **main ensemble technique** we will use! ğŸ­  \n",
    "- `DecisionTreeClassifier` is our **base learner** (small models that we combine in bagging). ğŸŒ³  \n",
    "- `accuracy_score` helps us **check how well our model performs**! âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“Š Step 2: Generate Sample Dataset**\n",
    "```python\n",
    "np.random.seed(42)  # ğŸ¯ Ensures the same random values every time you run the code\n",
    "\n",
    "X = np.random.rand(10, 3)  # ğŸ”¢ 10 rows, 3 features (random values between 0 and 1)\n",
    "y = np.random.randint(0, 2, 10)  # ğŸ¯ 10 random target values (0 or 1)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])  # ğŸ“ Create a DataFrame\n",
    "df[\"Target\"] = y  # ğŸ¯ Add target variable (0 or 1)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `np.random.seed(42)`: Ensures **reproducibility** (you always get the same random numbers). ğŸ¯  \n",
    "- `np.random.rand(10, 3)`: Creates a **matrix of 10 rows & 3 columns** with random numbers. ğŸ”¢  \n",
    "- `np.random.randint(0, 2, 10)`: Generates **binary target values (0 or 1)**. ğŸ¯  \n",
    "- `pd.DataFrame()`: Converts the array into a **structured table** for easy handling. ğŸ“  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ”€ Step 3: Split Data Into Train & Test Sets**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `train_test_split()`: **Splits** our dataset into **80% training** and **20% testing**. ğŸ”€  \n",
    "- `test_size=0.2`: Means **20% of the data** will be used for testing. ğŸ“Š  \n",
    "- `random_state=42`: Ensures **consistent** splits every time you run the code. ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ­ Step 4: Implement Bagging**\n",
    "```python\n",
    "base_estimator = DecisionTreeClassifier()  # ğŸŒ³ Create a simple Decision Tree\n",
    "\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=base_estimator,  # ğŸŒ± Weak learner (small model)\n",
    "    n_estimators=10,  # ğŸ”¢ Number of models (we use 10 trees)\n",
    "    max_samples=0.8,  # ğŸ“Š Each tree gets 80% of the data\n",
    "    bootstrap=True,  # ğŸ”„ Sampling with replacement\n",
    "    random_state=42  # ğŸ¯ Keep results consistent\n",
    ")\n",
    "\n",
    "bagging_clf.fit(X_train, y_train)  # ğŸ‹ï¸ Train the model\n",
    "y_pred_bagging = bagging_clf.predict(X_test)  # ğŸ§ Make predictions\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)  # âœ… Check accuracy\n",
    "\n",
    "print(\"ğŸ¯ Bagging Classifier Accuracy:\", accuracy_bagging)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "1. `DecisionTreeClassifier()`: ğŸŒ³  \n",
    "   - We create a **Decision Tree** model, which will act as our **weak learner**.  \n",
    "   - A **weak learner** is a model that **performs slightly better than random guessing**.  \n",
    "\n",
    "2. `BaggingClassifier(...)`: ğŸ­  \n",
    "   - `base_estimator=base_estimator`: Uses **Decision Tree** as the base model.  \n",
    "   - `n_estimators=10`: Creates **10 different trees** (each trained on different data). ğŸ”¢  \n",
    "   - `max_samples=0.8`: Each tree **only sees 80%** of the total data. ğŸ“Š  \n",
    "   - `bootstrap=True`: **Sampling with replacement** (some samples appear multiple times). ğŸ”„  \n",
    "   - `random_state=42`: Ensures **reproducibility**. ğŸ¯  \n",
    "\n",
    "3. `.fit(X_train, y_train)`: ğŸ‹ï¸  \n",
    "   - **Trains** the bagging model on the training data.  \n",
    "\n",
    "4. `.predict(X_test)`: ğŸ§  \n",
    "   - **Makes predictions** on the test data.  \n",
    "\n",
    "5. `accuracy_score(y_test, y_pred_bagging)`: âœ…  \n",
    "   - Compares **predicted values** with **actual values** to calculate accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ Final Output**\n",
    "When you run the code, youâ€™ll get something like:\n",
    "```\n",
    "ğŸ¯ Bagging Classifier Accuracy: 0.5\n",
    "```\n",
    "This means the **Bagging model correctly predicted 50% of test samples**. ğŸ“Š  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ§ Why Use Bagging?**\n",
    "âœ”ï¸ **Reduces Overfitting** ğŸ¤¯ â†’ By training multiple models and averaging results.  \n",
    "âœ”ï¸ **More Stability** ğŸ’ª â†’ One bad model wonâ€™t ruin everything!  \n",
    "âœ”ï¸ **Parallel Training** âš¡ â†’ All models are trained **independently**, making it fast.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ­ Summary**\n",
    "âœ… **Bagging** = Combining **multiple weak models** to form a **stronger model**.  \n",
    "âœ… We used **10 decision trees** ğŸŒ³ trained on **different subsets** of the data.  \n",
    "âœ… The **final prediction** is the **majority vote** of all trees. ğŸ—³ï¸  \n",
    "âœ… The **accuracy** tells us how well the model performs. ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **Thatâ€™s it! You now understand Bagging in depth!** ğŸ’¡ Let me know if you have any doubts. ğŸš€ğŸ”¥Alright! Letâ€™s focus only on **Bagging** and make the explanation super interactive with emojis! ğŸ§‘â€ğŸ’»âœ¨  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸš€ Step 1: Import Required Libraries**\n",
    "```python\n",
    "import numpy as np  # ğŸ“Š For numerical operations\n",
    "import pandas as pd  # ğŸ“ For data handling\n",
    "from sklearn.model_selection import train_test_split  # ğŸ”€ For splitting data\n",
    "from sklearn.ensemble import BaggingClassifier  # ğŸ­ Bagging model\n",
    "from sklearn.tree import DecisionTreeClassifier  # ğŸŒ³ Base model (Decision Tree)\n",
    "from sklearn.metrics import accuracy_score  # âœ… For evaluating model performance\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `numpy` (`np`) helps us generate random numbers for our dataset. ğŸ“Š  \n",
    "- `pandas` (`pd`) allows us to structure the dataset into a table. ğŸ“  \n",
    "- `train_test_split` will **divide** our data into **training** and **testing** sets. ğŸ”€  \n",
    "- `BaggingClassifier` is the **main ensemble technique** we will use! ğŸ­  \n",
    "- `DecisionTreeClassifier` is our **base learner** (small models that we combine in bagging). ğŸŒ³  \n",
    "- `accuracy_score` helps us **check how well our model performs**! âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“Š Step 2: Generate Sample Dataset**\n",
    "```python\n",
    "np.random.seed(42)  # ğŸ¯ Ensures the same random values every time you run the code\n",
    "\n",
    "X = np.random.rand(10, 3)  # ğŸ”¢ 10 rows, 3 features (random values between 0 and 1)\n",
    "y = np.random.randint(0, 2, 10)  # ğŸ¯ 10 random target values (0 or 1)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])  # ğŸ“ Create a DataFrame\n",
    "df[\"Target\"] = y  # ğŸ¯ Add target variable (0 or 1)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `np.random.seed(42)`: Ensures **reproducibility** (you always get the same random numbers). ğŸ¯  \n",
    "- `np.random.rand(10, 3)`: Creates a **matrix of 10 rows & 3 columns** with random numbers. ğŸ”¢  \n",
    "- `np.random.randint(0, 2, 10)`: Generates **binary target values (0 or 1)**. ğŸ¯  \n",
    "- `pd.DataFrame()`: Converts the array into a **structured table** for easy handling. ğŸ“  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ”€ Step 3: Split Data Into Train & Test Sets**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `train_test_split()`: **Splits** our dataset into **80% training** and **20% testing**. ğŸ”€  \n",
    "- `test_size=0.2`: Means **20% of the data** will be used for testing. ğŸ“Š  \n",
    "- `random_state=42`: Ensures **consistent** splits every time you run the code. ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ­ Step 4: Implement Bagging**\n",
    "```python\n",
    "base_estimator = DecisionTreeClassifier()  # ğŸŒ³ Create a simple Decision Tree\n",
    "\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=base_estimator,  # ğŸŒ± Weak learner (small model)\n",
    "    n_estimators=10,  # ğŸ”¢ Number of models (we use 10 trees)\n",
    "    max_samples=0.8,  # ğŸ“Š Each tree gets 80% of the data\n",
    "    bootstrap=True,  # ğŸ”„ Sampling with replacement\n",
    "    random_state=42  # ğŸ¯ Keep results consistent\n",
    ")\n",
    "\n",
    "bagging_clf.fit(X_train, y_train)  # ğŸ‹ï¸ Train the model\n",
    "y_pred_bagging = bagging_clf.predict(X_test)  # ğŸ§ Make predictions\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)  # âœ… Check accuracy\n",
    "\n",
    "print(\"ğŸ¯ Bagging Classifier Accuracy:\", accuracy_bagging)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "1. `DecisionTreeClassifier()`: ğŸŒ³  \n",
    "   - We create a **Decision Tree** model, which will act as our **weak learner**.  \n",
    "   - A **weak learner** is a model that **performs slightly better than random guessing**.  \n",
    "\n",
    "2. `BaggingClassifier(...)`: ğŸ­  \n",
    "   - `base_estimator=base_estimator`: Uses **Decision Tree** as the base model.  \n",
    "   - `n_estimators=10`: Creates **10 different trees** (each trained on different data). ğŸ”¢  \n",
    "   - `max_samples=0.8`: Each tree **only sees 80%** of the total data. ğŸ“Š  \n",
    "   - `bootstrap=True`: **Sampling with replacement** (some samples appear multiple times). ğŸ”„  \n",
    "   - `random_state=42`: Ensures **reproducibility**. ğŸ¯  \n",
    "\n",
    "3. `.fit(X_train, y_train)`: ğŸ‹ï¸  \n",
    "   - **Trains** the bagging model on the training data.  \n",
    "\n",
    "4. `.predict(X_test)`: ğŸ§  \n",
    "   - **Makes predictions** on the test data.  \n",
    "\n",
    "5. `accuracy_score(y_test, y_pred_bagging)`: âœ…  \n",
    "   - Compares **predicted values** with **actual values** to calculate accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ Final Output**\n",
    "When you run the code, youâ€™ll get something like:\n",
    "```\n",
    "ğŸ¯ Bagging Classifier Accuracy: 0.5\n",
    "```\n",
    "This means the **Bagging model correctly predicted 50% of test samples**. ğŸ“Š  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ§ Why Use Bagging?**\n",
    "âœ”ï¸ **Reduces Overfitting** ğŸ¤¯ â†’ By training multiple models and averaging results.  \n",
    "âœ”ï¸ **More Stability** ğŸ’ª â†’ One bad model wonâ€™t ruin everything!  \n",
    "âœ”ï¸ **Parallel Training** âš¡ â†’ All models are trained **independently**, making it fast.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ­ Summary**\n",
    "âœ… **Bagging** = Combining **multiple weak models** to form a **stronger model**.  \n",
    "âœ… We used **10 decision trees** ğŸŒ³ trained on **different subsets** of the data.  \n",
    "âœ… The **final prediction** is the **majority vote** of all trees. ğŸ—³ï¸  \n",
    "âœ… The **accuracy** tells us how well the model performs. ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰  ğŸš€ğŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
