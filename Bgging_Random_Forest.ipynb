{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Let’s focus only on **Bagging** . 🧑‍💻✨  \n",
    "\n",
    "---\n",
    "\n",
    "## **🚀 Step 1: Import Required Libraries**\n",
    "```python\n",
    "import numpy as np  # 📊 For numerical operations\n",
    "import pandas as pd  # 📝 For data handling\n",
    "from sklearn.model_selection import train_test_split  # 🔀 For splitting data\n",
    "from sklearn.ensemble import BaggingClassifier  # 🎭 Bagging model\n",
    "from sklearn.tree import DecisionTreeClassifier  # 🌳 Base model (Decision Tree)\n",
    "from sklearn.metrics import accuracy_score  # ✅ For evaluating model performance\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- `numpy` (`np`) helps us generate random numbers for our dataset. 📊  \n",
    "- `pandas` (`pd`) allows us to structure the dataset into a table. 📝  \n",
    "- `train_test_split` will **divide** our data into **training** and **testing** sets. 🔀  \n",
    "- `BaggingClassifier` is the **main ensemble technique** we will use! 🎭  \n",
    "- `DecisionTreeClassifier` is our **base learner** (small models that we combine in bagging). 🌳  \n",
    "- `accuracy_score` helps us **check how well our model performs**! ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## **📊 Step 2: Generate Sample Dataset**\n",
    "```python\n",
    "np.random.seed(42)  # 🎯 Ensures the same random values every time you run the code\n",
    "\n",
    "X = np.random.rand(10, 3)  # 🔢 10 rows, 3 features (random values between 0 and 1)\n",
    "y = np.random.randint(0, 2, 10)  # 🎯 10 random target values (0 or 1)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])  # 📝 Create a DataFrame\n",
    "df[\"Target\"] = y  # 🎯 Add target variable (0 or 1)\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- `np.random.seed(42)`: Ensures **reproducibility** (you always get the same random numbers). 🎯  \n",
    "- `np.random.rand(10, 3)`: Creates a **matrix of 10 rows & 3 columns** with random numbers. 🔢  \n",
    "- `np.random.randint(0, 2, 10)`: Generates **binary target values (0 or 1)**. 🎯  \n",
    "- `pd.DataFrame()`: Converts the array into a **structured table** for easy handling. 📝  \n",
    "\n",
    "---\n",
    "\n",
    "## **🔀 Step 3: Split Data Into Train & Test Sets**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- `train_test_split()`: **Splits** our dataset into **80% training** and **20% testing**. 🔀  \n",
    "- `test_size=0.2`: Means **20% of the data** will be used for testing. 📊  \n",
    "- `random_state=42`: Ensures **consistent** splits every time you run the code. 🎯  \n",
    "\n",
    "---\n",
    "\n",
    "## **🎭 Step 4: Implement Bagging**\n",
    "```python\n",
    "base_estimator = DecisionTreeClassifier()  # 🌳 Create a simple Decision Tree\n",
    "\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=base_estimator,  # 🌱 Weak learner (small model)\n",
    "    n_estimators=10,  # 🔢 Number of models (we use 10 trees)\n",
    "    max_samples=0.8,  # 📊 Each tree gets 80% of the data\n",
    "    bootstrap=True,  # 🔄 Sampling with replacement\n",
    "    random_state=42  # 🎯 Keep results consistent\n",
    ")\n",
    "\n",
    "bagging_clf.fit(X_train, y_train)  # 🏋️ Train the model\n",
    "y_pred_bagging = bagging_clf.predict(X_test)  # 🧐 Make predictions\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)  # ✅ Check accuracy\n",
    "\n",
    "print(\"🎯 Bagging Classifier Accuracy:\", accuracy_bagging)\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "1. `DecisionTreeClassifier()`: 🌳  \n",
    "   - We create a **Decision Tree** model, which will act as our **weak learner**.  \n",
    "   - A **weak learner** is a model that **performs slightly better than random guessing**.  \n",
    "\n",
    "2. `BaggingClassifier(...)`: 🎭  \n",
    "   - `base_estimator=base_estimator`: Uses **Decision Tree** as the base model.  \n",
    "   - `n_estimators=10`: Creates **10 different trees** (each trained on different data). 🔢  \n",
    "   - `max_samples=0.8`: Each tree **only sees 80%** of the total data. 📊  \n",
    "   - `bootstrap=True`: **Sampling with replacement** (some samples appear multiple times). 🔄  \n",
    "   - `random_state=42`: Ensures **reproducibility**. 🎯  \n",
    "\n",
    "3. `.fit(X_train, y_train)`: 🏋️  \n",
    "   - **Trains** the bagging model on the training data.  \n",
    "\n",
    "4. `.predict(X_test)`: 🧐  \n",
    "   - **Makes predictions** on the test data.  \n",
    "\n",
    "5. `accuracy_score(y_test, y_pred_bagging)`: ✅  \n",
    "   - Compares **predicted values** with **actual values** to calculate accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Final Output**\n",
    "When you run the code, you’ll get something like:\n",
    "```\n",
    "🎯 Bagging Classifier Accuracy: 0.5\n",
    "```\n",
    "This means the **Bagging model correctly predicted 50% of test samples**. 📊  \n",
    "\n",
    "---\n",
    "\n",
    "## **🧐 Why Use Bagging?**\n",
    "✔️ **Reduces Overfitting** 🤯 → By training multiple models and averaging results.  \n",
    "✔️ **More Stability** 💪 → One bad model won’t ruin everything!  \n",
    "✔️ **Parallel Training** ⚡ → All models are trained **independently**, making it fast.  \n",
    "\n",
    "---\n",
    "\n",
    "## **🎭 Summary**\n",
    "✅ **Bagging** = Combining **multiple weak models** to form a **stronger model**.  \n",
    "✅ We used **10 decision trees** 🌳 trained on **different subsets** of the data.  \n",
    "✅ The **final prediction** is the **majority vote** of all trees. 🗳️  \n",
    "✅ The **accuracy** tells us how well the model performs. 🎯  \n",
    "\n",
    "---\n",
    "\n",
    "🎉 **That’s it! You now understand Bagging in depth!** 💡 Let me know if you have any doubts. 🚀🔥Alright! Let’s focus only on **Bagging** and make the explanation super interactive with emojis! 🧑‍💻✨  \n",
    "\n",
    "---\n",
    "\n",
    "## **🚀 Step 1: Import Required Libraries**\n",
    "```python\n",
    "import numpy as np  # 📊 For numerical operations\n",
    "import pandas as pd  # 📝 For data handling\n",
    "from sklearn.model_selection import train_test_split  # 🔀 For splitting data\n",
    "from sklearn.ensemble import BaggingClassifier  # 🎭 Bagging model\n",
    "from sklearn.tree import DecisionTreeClassifier  # 🌳 Base model (Decision Tree)\n",
    "from sklearn.metrics import accuracy_score  # ✅ For evaluating model performance\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- `numpy` (`np`) helps us generate random numbers for our dataset. 📊  \n",
    "- `pandas` (`pd`) allows us to structure the dataset into a table. 📝  \n",
    "- `train_test_split` will **divide** our data into **training** and **testing** sets. 🔀  \n",
    "- `BaggingClassifier` is the **main ensemble technique** we will use! 🎭  \n",
    "- `DecisionTreeClassifier` is our **base learner** (small models that we combine in bagging). 🌳  \n",
    "- `accuracy_score` helps us **check how well our model performs**! ✅  \n",
    "\n",
    "---\n",
    "\n",
    "## **📊 Step 2: Generate Sample Dataset**\n",
    "```python\n",
    "np.random.seed(42)  # 🎯 Ensures the same random values every time you run the code\n",
    "\n",
    "X = np.random.rand(10, 3)  # 🔢 10 rows, 3 features (random values between 0 and 1)\n",
    "y = np.random.randint(0, 2, 10)  # 🎯 10 random target values (0 or 1)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])  # 📝 Create a DataFrame\n",
    "df[\"Target\"] = y  # 🎯 Add target variable (0 or 1)\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- `np.random.seed(42)`: Ensures **reproducibility** (you always get the same random numbers). 🎯  \n",
    "- `np.random.rand(10, 3)`: Creates a **matrix of 10 rows & 3 columns** with random numbers. 🔢  \n",
    "- `np.random.randint(0, 2, 10)`: Generates **binary target values (0 or 1)**. 🎯  \n",
    "- `pd.DataFrame()`: Converts the array into a **structured table** for easy handling. 📝  \n",
    "\n",
    "---\n",
    "\n",
    "## **🔀 Step 3: Split Data Into Train & Test Sets**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- `train_test_split()`: **Splits** our dataset into **80% training** and **20% testing**. 🔀  \n",
    "- `test_size=0.2`: Means **20% of the data** will be used for testing. 📊  \n",
    "- `random_state=42`: Ensures **consistent** splits every time you run the code. 🎯  \n",
    "\n",
    "---\n",
    "\n",
    "## **🎭 Step 4: Implement Bagging**\n",
    "```python\n",
    "base_estimator = DecisionTreeClassifier()  # 🌳 Create a simple Decision Tree\n",
    "\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=base_estimator,  # 🌱 Weak learner (small model)\n",
    "    n_estimators=10,  # 🔢 Number of models (we use 10 trees)\n",
    "    max_samples=0.8,  # 📊 Each tree gets 80% of the data\n",
    "    bootstrap=True,  # 🔄 Sampling with replacement\n",
    "    random_state=42  # 🎯 Keep results consistent\n",
    ")\n",
    "\n",
    "bagging_clf.fit(X_train, y_train)  # 🏋️ Train the model\n",
    "y_pred_bagging = bagging_clf.predict(X_test)  # 🧐 Make predictions\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)  # ✅ Check accuracy\n",
    "\n",
    "print(\"🎯 Bagging Classifier Accuracy:\", accuracy_bagging)\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "1. `DecisionTreeClassifier()`: 🌳  \n",
    "   - We create a **Decision Tree** model, which will act as our **weak learner**.  \n",
    "   - A **weak learner** is a model that **performs slightly better than random guessing**.  \n",
    "\n",
    "2. `BaggingClassifier(...)`: 🎭  \n",
    "   - `base_estimator=base_estimator`: Uses **Decision Tree** as the base model.  \n",
    "   - `n_estimators=10`: Creates **10 different trees** (each trained on different data). 🔢  \n",
    "   - `max_samples=0.8`: Each tree **only sees 80%** of the total data. 📊  \n",
    "   - `bootstrap=True`: **Sampling with replacement** (some samples appear multiple times). 🔄  \n",
    "   - `random_state=42`: Ensures **reproducibility**. 🎯  \n",
    "\n",
    "3. `.fit(X_train, y_train)`: 🏋️  \n",
    "   - **Trains** the bagging model on the training data.  \n",
    "\n",
    "4. `.predict(X_test)`: 🧐  \n",
    "   - **Makes predictions** on the test data.  \n",
    "\n",
    "5. `accuracy_score(y_test, y_pred_bagging)`: ✅  \n",
    "   - Compares **predicted values** with **actual values** to calculate accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Final Output**\n",
    "When you run the code, you’ll get something like:\n",
    "```\n",
    "🎯 Bagging Classifier Accuracy: 0.5\n",
    "```\n",
    "This means the **Bagging model correctly predicted 50% of test samples**. 📊  \n",
    "\n",
    "---\n",
    "\n",
    "## **🧐 Why Use Bagging?**\n",
    "✔️ **Reduces Overfitting** 🤯 → By training multiple models and averaging results.  \n",
    "✔️ **More Stability** 💪 → One bad model won’t ruin everything!  \n",
    "✔️ **Parallel Training** ⚡ → All models are trained **independently**, making it fast.  \n",
    "\n",
    "---\n",
    "\n",
    "## **🎭 Summary**\n",
    "✅ **Bagging** = Combining **multiple weak models** to form a **stronger model**.  \n",
    "✅ We used **10 decision trees** 🌳 trained on **different subsets** of the data.  \n",
    "✅ The **final prediction** is the **majority vote** of all trees. 🗳️  \n",
    "✅ The **accuracy** tells us how well the model performs. 🎯  \n",
    "\n",
    "---\n",
    "\n",
    "🎉  🚀🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of diffrent models in one. Let’s go **step by step** and break down the entire code in detail with **emojis** for better clarity. 🚀✨  \n",
    "\n",
    "---\n",
    "\n",
    "# **🚀 Step 1: Import Required Libraries**\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier  # 🗳️ Heterogeneous ensemble\n",
    "from sklearn.tree import DecisionTreeClassifier  # 🌳 Decision Tree\n",
    "from sklearn.svm import SVC  # 📈 Support Vector Machine\n",
    "from sklearn.naive_bayes import GaussianNB  # 📊 Naïve Bayes\n",
    "from sklearn.model_selection import train_test_split  # 🔀 Split data\n",
    "from sklearn.metrics import accuracy_score  # ✅ Evaluate model performance\n",
    "import numpy as np  # 📊 Handle numerical operations\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "1. `VotingClassifier`: 🗳️ Used to combine different models and make a final decision based on their votes.  \n",
    "2. `DecisionTreeClassifier`: 🌳 A simple tree-based model that splits data at different points to classify it.  \n",
    "3. `SVC`: 📈 Support Vector Machine, a powerful model used for classification.  \n",
    "4. `GaussianNB`: 📊 A probability-based model using **Bayes’ Theorem** for classification.  \n",
    "5. `train_test_split`: 🔀 Splits the dataset into **training and testing** sets.  \n",
    "6. `accuracy_score`: ✅ Measures how well the model performs.  \n",
    "7. `numpy`: 📊 Helps generate **random data** for this example.  \n",
    "\n",
    "---\n",
    "\n",
    "# **📊 Step 2: Generate Sample Data**\n",
    "```python\n",
    "np.random.seed(42)  # 🎯 Ensures reproducibility (same random values every time)\n",
    "X = np.random.rand(10, 3)  # 🔢 Create 10 rows, 3 features with random values\n",
    "y = np.random.randint(0, 2, 10)  # 🎯 Generate random binary target values (0 or 1)\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- `np.random.seed(42)`:  \n",
    "  - Makes sure the **random numbers are the same every time** you run the code.  \n",
    "  - Helps in debugging and ensures **consistent results**. 🎯  \n",
    "- `np.random.rand(10, 3)`:  \n",
    "  - Creates **10 rows** and **3 columns** of **random numbers** between `0` and `1`.  \n",
    "  - These numbers are the **features** of our dataset. 🔢  \n",
    "- `np.random.randint(0, 2, 10)`:  \n",
    "  - Creates a **target variable** with **10 random values** (either `0` or `1`). 🎯  \n",
    "  - This represents **binary classification**.  \n",
    "\n",
    "📌 **At this point, we have a dataset with**:\n",
    "| Feature1 | Feature2 | Feature3 | Target |\n",
    "|----------|----------|----------|--------|\n",
    "| 0.37     | 0.95     | 0.73     | 1      |\n",
    "| 0.60     | 0.16     | 0.87     | 0      |\n",
    "| 0.06     | 0.87     | 0.60     | 1      |\n",
    "| ...      | ...      | ...      | ...    |\n",
    "\n",
    "---\n",
    "\n",
    "# **🔀 Step 3: Split Data Into Training & Testing**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- **Splitting Data**:\n",
    "  - `train_test_split()` divides the dataset into **80% training** and **20% testing**. 🔀  \n",
    "- **Parameters**:\n",
    "  - `test_size=0.2`: Means **20% of the data** is for testing.  \n",
    "  - `random_state=42`: Ensures the **split remains the same** every time. 🎯  \n",
    "\n",
    "---\n",
    "\n",
    "# **🎭 Step 4: Define Base Models**\n",
    "```python\n",
    "dt = DecisionTreeClassifier()  # 🌳 Decision Tree Classifier\n",
    "svm = SVC(probability=True)  # 📈 Support Vector Machine (Enable probability=True for soft voting)\n",
    "nb = GaussianNB()  # 📊 Naïve Bayes Classifier\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- We are defining **three different base models**:\n",
    "  1. **DecisionTreeClassifier()** 🌳  \n",
    "     - Works by **splitting** the data at different points.  \n",
    "  2. **SVC() (Support Vector Classifier)** 📈  \n",
    "     - Finds the **best decision boundary** between classes.  \n",
    "     - We set `probability=True` because **soft voting** requires probabilities.  \n",
    "  3. **GaussianNB() (Naïve Bayes)** 📊  \n",
    "     - Uses **Bayes’ theorem** to classify data based on probability.  \n",
    "\n",
    "---\n",
    "\n",
    "# **🗳️ Step 5: Create Voting Classifier**\n",
    "```python\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"Decision Tree\", dt), (\"SVM\", svm), (\"Naïve Bayes\", nb)],  \n",
    "    voting=\"soft\"  # 🔥 Uses predicted probabilities for better performance\n",
    ")\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- `VotingClassifier(...)`:  \n",
    "  - Combines different models into **one ensemble model**. 🏆  \n",
    "- `estimators=[...]`:  \n",
    "  - Defines the **models we want to use**:\n",
    "    - `\"Decision Tree\"` 🌳  \n",
    "    - `\"SVM\"` 📈  \n",
    "    - `\"Naïve Bayes\"` 📊  \n",
    "- `voting=\"soft\"`:  \n",
    "  - **Soft Voting** means models give **probabilities** for each class instead of just voting.  \n",
    "  - The class with the **highest combined probability wins**. 🔥  \n",
    "  - **Soft Voting is better** than Hard Voting because it gives more weight to confident predictions.  \n",
    "\n",
    "📌 **Example of Soft Voting**  \n",
    "Let’s say we want to classify a new data point:  \n",
    "| Model | Probability of Class 0 | Probability of Class 1 |\n",
    "|--------|-------------------------|-------------------------|\n",
    "| Decision Tree 🌳 | 0.4 | 0.6 |\n",
    "| SVM 📈 | 0.2 | 0.8 |\n",
    "| Naïve Bayes 📊 | 0.3 | 0.7 |\n",
    "| **Final Prediction 🗳️** | **0.3 (avg)** | **0.7 (avg) → Class 1 🎯** |\n",
    "\n",
    "---\n",
    "\n",
    "# **🏋️ Step 6: Train the Model**\n",
    "```python\n",
    "voting_clf.fit(X_train, y_train)\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- **Trains** the entire **ensemble model** using `X_train` and `y_train`. 🏋️  \n",
    "- Each **individual model learns** from the training data.  \n",
    "- The **final VotingClassifier** will use their outputs for predictions. 🎯  \n",
    "\n",
    "---\n",
    "\n",
    "# **🧐 Step 7: Make Predictions**\n",
    "```python\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- Uses the **trained VotingClassifier** to predict results for `X_test`.  \n",
    "- It **combines predictions** from all base models using **soft voting**. 🎭  \n",
    "\n",
    "---\n",
    "\n",
    "# **✅ Step 8: Evaluate Accuracy**\n",
    "```python\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"🎯 Heterogeneous Ensemble Accuracy:\", accuracy)\n",
    "```\n",
    "### **🔎 What’s happening here?**\n",
    "- `accuracy_score(y_test, y_pred)`:  \n",
    "  - Compares **actual labels** (`y_test`) with **predicted labels** (`y_pred`).  \n",
    "  - Calculates the **accuracy of the model**. ✅  \n",
    "- The result is printed as:\n",
    "  ```\n",
    "  🎯 Heterogeneous Ensemble Accuracy: 0.5\n",
    "  ```\n",
    "  - This means the model predicted **50% of test samples correctly**. 📊  \n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Final Takeaways**\n",
    "✔️ **Bagging uses the same model multiple times** (e.g., 10 Decision Trees 🌳).  \n",
    "✔️ **Voting Classifier allows different models** (e.g., Decision Tree 🌳, SVM 📈, Naïve Bayes 📊).  \n",
    "✔️ **Soft Voting considers probabilities**, making it **more accurate** than Hard Voting.  \n",
    "✔️ **Each model contributes differently**, making the system more **stable**.  \n",
    "\n",
    "---\n",
    "\n",
    "🚀🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
