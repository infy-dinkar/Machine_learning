{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Letâ€™s focus only on **Bagging** . ğŸ§‘â€ğŸ’»âœ¨  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸš€ Step 1: Import Required Libraries**\n",
    "```python\n",
    "import numpy as np  # ğŸ“Š For numerical operations\n",
    "import pandas as pd  # ğŸ“ For data handling\n",
    "from sklearn.model_selection import train_test_split  # ğŸ”€ For splitting data\n",
    "from sklearn.ensemble import BaggingClassifier  # ğŸ­ Bagging model\n",
    "from sklearn.tree import DecisionTreeClassifier  # ğŸŒ³ Base model (Decision Tree)\n",
    "from sklearn.metrics import accuracy_score  # âœ… For evaluating model performance\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `numpy` (`np`) helps us generate random numbers for our dataset. ğŸ“Š  \n",
    "- `pandas` (`pd`) allows us to structure the dataset into a table. ğŸ“  \n",
    "- `train_test_split` will **divide** our data into **training** and **testing** sets. ğŸ”€  \n",
    "- `BaggingClassifier` is the **main ensemble technique** we will use! ğŸ­  \n",
    "- `DecisionTreeClassifier` is our **base learner** (small models that we combine in bagging). ğŸŒ³  \n",
    "- `accuracy_score` helps us **check how well our model performs**! âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“Š Step 2: Generate Sample Dataset**\n",
    "```python\n",
    "np.random.seed(42)  # ğŸ¯ Ensures the same random values every time you run the code\n",
    "\n",
    "X = np.random.rand(10, 3)  # ğŸ”¢ 10 rows, 3 features (random values between 0 and 1)\n",
    "y = np.random.randint(0, 2, 10)  # ğŸ¯ 10 random target values (0 or 1)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])  # ğŸ“ Create a DataFrame\n",
    "df[\"Target\"] = y  # ğŸ¯ Add target variable (0 or 1)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `np.random.seed(42)`: Ensures **reproducibility** (you always get the same random numbers). ğŸ¯  \n",
    "- `np.random.rand(10, 3)`: Creates a **matrix of 10 rows & 3 columns** with random numbers. ğŸ”¢  \n",
    "- `np.random.randint(0, 2, 10)`: Generates **binary target values (0 or 1)**. ğŸ¯  \n",
    "- `pd.DataFrame()`: Converts the array into a **structured table** for easy handling. ğŸ“  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ”€ Step 3: Split Data Into Train & Test Sets**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `train_test_split()`: **Splits** our dataset into **80% training** and **20% testing**. ğŸ”€  \n",
    "- `test_size=0.2`: Means **20% of the data** will be used for testing. ğŸ“Š  \n",
    "- `random_state=42`: Ensures **consistent** splits every time you run the code. ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ­ Step 4: Implement Bagging**\n",
    "```python\n",
    "base_estimator = DecisionTreeClassifier()  # ğŸŒ³ Create a simple Decision Tree\n",
    "\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=base_estimator,  # ğŸŒ± Weak learner (small model)\n",
    "    n_estimators=10,  # ğŸ”¢ Number of models (we use 10 trees)\n",
    "    max_samples=0.8,  # ğŸ“Š Each tree gets 80% of the data\n",
    "    bootstrap=True,  # ğŸ”„ Sampling with replacement\n",
    "    random_state=42  # ğŸ¯ Keep results consistent\n",
    ")\n",
    "\n",
    "bagging_clf.fit(X_train, y_train)  # ğŸ‹ï¸ Train the model\n",
    "y_pred_bagging = bagging_clf.predict(X_test)  # ğŸ§ Make predictions\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)  # âœ… Check accuracy\n",
    "\n",
    "print(\"ğŸ¯ Bagging Classifier Accuracy:\", accuracy_bagging)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "1. `DecisionTreeClassifier()`: ğŸŒ³  \n",
    "   - We create a **Decision Tree** model, which will act as our **weak learner**.  \n",
    "   - A **weak learner** is a model that **performs slightly better than random guessing**.  \n",
    "\n",
    "2. `BaggingClassifier(...)`: ğŸ­  \n",
    "   - `base_estimator=base_estimator`: Uses **Decision Tree** as the base model.  \n",
    "   - `n_estimators=10`: Creates **10 different trees** (each trained on different data). ğŸ”¢  \n",
    "   - `max_samples=0.8`: Each tree **only sees 80%** of the total data. ğŸ“Š  \n",
    "   - `bootstrap=True`: **Sampling with replacement** (some samples appear multiple times). ğŸ”„  \n",
    "   - `random_state=42`: Ensures **reproducibility**. ğŸ¯  \n",
    "\n",
    "3. `.fit(X_train, y_train)`: ğŸ‹ï¸  \n",
    "   - **Trains** the bagging model on the training data.  \n",
    "\n",
    "4. `.predict(X_test)`: ğŸ§  \n",
    "   - **Makes predictions** on the test data.  \n",
    "\n",
    "5. `accuracy_score(y_test, y_pred_bagging)`: âœ…  \n",
    "   - Compares **predicted values** with **actual values** to calculate accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ Final Output**\n",
    "When you run the code, youâ€™ll get something like:\n",
    "```\n",
    "ğŸ¯ Bagging Classifier Accuracy: 0.5\n",
    "```\n",
    "This means the **Bagging model correctly predicted 50% of test samples**. ğŸ“Š  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ§ Why Use Bagging?**\n",
    "âœ”ï¸ **Reduces Overfitting** ğŸ¤¯ â†’ By training multiple models and averaging results.  \n",
    "âœ”ï¸ **More Stability** ğŸ’ª â†’ One bad model wonâ€™t ruin everything!  \n",
    "âœ”ï¸ **Parallel Training** âš¡ â†’ All models are trained **independently**, making it fast.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ­ Summary**\n",
    "âœ… **Bagging** = Combining **multiple weak models** to form a **stronger model**.  \n",
    "âœ… We used **10 decision trees** ğŸŒ³ trained on **different subsets** of the data.  \n",
    "âœ… The **final prediction** is the **majority vote** of all trees. ğŸ—³ï¸  \n",
    "âœ… The **accuracy** tells us how well the model performs. ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰ **Thatâ€™s it! You now understand Bagging in depth!** ğŸ’¡ Let me know if you have any doubts. ğŸš€ğŸ”¥Alright! Letâ€™s focus only on **Bagging** and make the explanation super interactive with emojis! ğŸ§‘â€ğŸ’»âœ¨  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸš€ Step 1: Import Required Libraries**\n",
    "```python\n",
    "import numpy as np  # ğŸ“Š For numerical operations\n",
    "import pandas as pd  # ğŸ“ For data handling\n",
    "from sklearn.model_selection import train_test_split  # ğŸ”€ For splitting data\n",
    "from sklearn.ensemble import BaggingClassifier  # ğŸ­ Bagging model\n",
    "from sklearn.tree import DecisionTreeClassifier  # ğŸŒ³ Base model (Decision Tree)\n",
    "from sklearn.metrics import accuracy_score  # âœ… For evaluating model performance\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `numpy` (`np`) helps us generate random numbers for our dataset. ğŸ“Š  \n",
    "- `pandas` (`pd`) allows us to structure the dataset into a table. ğŸ“  \n",
    "- `train_test_split` will **divide** our data into **training** and **testing** sets. ğŸ”€  \n",
    "- `BaggingClassifier` is the **main ensemble technique** we will use! ğŸ­  \n",
    "- `DecisionTreeClassifier` is our **base learner** (small models that we combine in bagging). ğŸŒ³  \n",
    "- `accuracy_score` helps us **check how well our model performs**! âœ…  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ“Š Step 2: Generate Sample Dataset**\n",
    "```python\n",
    "np.random.seed(42)  # ğŸ¯ Ensures the same random values every time you run the code\n",
    "\n",
    "X = np.random.rand(10, 3)  # ğŸ”¢ 10 rows, 3 features (random values between 0 and 1)\n",
    "y = np.random.randint(0, 2, 10)  # ğŸ¯ 10 random target values (0 or 1)\n",
    "\n",
    "df = pd.DataFrame(X, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])  # ğŸ“ Create a DataFrame\n",
    "df[\"Target\"] = y  # ğŸ¯ Add target variable (0 or 1)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `np.random.seed(42)`: Ensures **reproducibility** (you always get the same random numbers). ğŸ¯  \n",
    "- `np.random.rand(10, 3)`: Creates a **matrix of 10 rows & 3 columns** with random numbers. ğŸ”¢  \n",
    "- `np.random.randint(0, 2, 10)`: Generates **binary target values (0 or 1)**. ğŸ¯  \n",
    "- `pd.DataFrame()`: Converts the array into a **structured table** for easy handling. ğŸ“  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ”€ Step 3: Split Data Into Train & Test Sets**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `train_test_split()`: **Splits** our dataset into **80% training** and **20% testing**. ğŸ”€  \n",
    "- `test_size=0.2`: Means **20% of the data** will be used for testing. ğŸ“Š  \n",
    "- `random_state=42`: Ensures **consistent** splits every time you run the code. ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ­ Step 4: Implement Bagging**\n",
    "```python\n",
    "base_estimator = DecisionTreeClassifier()  # ğŸŒ³ Create a simple Decision Tree\n",
    "\n",
    "bagging_clf = BaggingClassifier(\n",
    "    base_estimator=base_estimator,  # ğŸŒ± Weak learner (small model)\n",
    "    n_estimators=10,  # ğŸ”¢ Number of models (we use 10 trees)\n",
    "    max_samples=0.8,  # ğŸ“Š Each tree gets 80% of the data\n",
    "    bootstrap=True,  # ğŸ”„ Sampling with replacement\n",
    "    random_state=42  # ğŸ¯ Keep results consistent\n",
    ")\n",
    "\n",
    "bagging_clf.fit(X_train, y_train)  # ğŸ‹ï¸ Train the model\n",
    "y_pred_bagging = bagging_clf.predict(X_test)  # ğŸ§ Make predictions\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)  # âœ… Check accuracy\n",
    "\n",
    "print(\"ğŸ¯ Bagging Classifier Accuracy:\", accuracy_bagging)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "1. `DecisionTreeClassifier()`: ğŸŒ³  \n",
    "   - We create a **Decision Tree** model, which will act as our **weak learner**.  \n",
    "   - A **weak learner** is a model that **performs slightly better than random guessing**.  \n",
    "\n",
    "2. `BaggingClassifier(...)`: ğŸ­  \n",
    "   - `base_estimator=base_estimator`: Uses **Decision Tree** as the base model.  \n",
    "   - `n_estimators=10`: Creates **10 different trees** (each trained on different data). ğŸ”¢  \n",
    "   - `max_samples=0.8`: Each tree **only sees 80%** of the total data. ğŸ“Š  \n",
    "   - `bootstrap=True`: **Sampling with replacement** (some samples appear multiple times). ğŸ”„  \n",
    "   - `random_state=42`: Ensures **reproducibility**. ğŸ¯  \n",
    "\n",
    "3. `.fit(X_train, y_train)`: ğŸ‹ï¸  \n",
    "   - **Trains** the bagging model on the training data.  \n",
    "\n",
    "4. `.predict(X_test)`: ğŸ§  \n",
    "   - **Makes predictions** on the test data.  \n",
    "\n",
    "5. `accuracy_score(y_test, y_pred_bagging)`: âœ…  \n",
    "   - Compares **predicted values** with **actual values** to calculate accuracy.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ Final Output**\n",
    "When you run the code, youâ€™ll get something like:\n",
    "```\n",
    "ğŸ¯ Bagging Classifier Accuracy: 0.5\n",
    "```\n",
    "This means the **Bagging model correctly predicted 50% of test samples**. ğŸ“Š  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ§ Why Use Bagging?**\n",
    "âœ”ï¸ **Reduces Overfitting** ğŸ¤¯ â†’ By training multiple models and averaging results.  \n",
    "âœ”ï¸ **More Stability** ğŸ’ª â†’ One bad model wonâ€™t ruin everything!  \n",
    "âœ”ï¸ **Parallel Training** âš¡ â†’ All models are trained **independently**, making it fast.  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ­ Summary**\n",
    "âœ… **Bagging** = Combining **multiple weak models** to form a **stronger model**.  \n",
    "âœ… We used **10 decision trees** ğŸŒ³ trained on **different subsets** of the data.  \n",
    "âœ… The **final prediction** is the **majority vote** of all trees. ğŸ—³ï¸  \n",
    "âœ… The **accuracy** tells us how well the model performs. ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸ‰  ğŸš€ğŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of diffrent models in one. Letâ€™s go **step by step** and break down the entire code in detail with **emojis** for better clarity. ğŸš€âœ¨  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸš€ Step 1: Import Required Libraries**\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier  # ğŸ—³ï¸ Heterogeneous ensemble\n",
    "from sklearn.tree import DecisionTreeClassifier  # ğŸŒ³ Decision Tree\n",
    "from sklearn.svm import SVC  # ğŸ“ˆ Support Vector Machine\n",
    "from sklearn.naive_bayes import GaussianNB  # ğŸ“Š NaÃ¯ve Bayes\n",
    "from sklearn.model_selection import train_test_split  # ğŸ”€ Split data\n",
    "from sklearn.metrics import accuracy_score  # âœ… Evaluate model performance\n",
    "import numpy as np  # ğŸ“Š Handle numerical operations\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "1. `VotingClassifier`: ğŸ—³ï¸ Used to combine different models and make a final decision based on their votes.  \n",
    "2. `DecisionTreeClassifier`: ğŸŒ³ A simple tree-based model that splits data at different points to classify it.  \n",
    "3. `SVC`: ğŸ“ˆ Support Vector Machine, a powerful model used for classification.  \n",
    "4. `GaussianNB`: ğŸ“Š A probability-based model using **Bayesâ€™ Theorem** for classification.  \n",
    "5. `train_test_split`: ğŸ”€ Splits the dataset into **training and testing** sets.  \n",
    "6. `accuracy_score`: âœ… Measures how well the model performs.  \n",
    "7. `numpy`: ğŸ“Š Helps generate **random data** for this example.  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸ“Š Step 2: Generate Sample Data**\n",
    "```python\n",
    "np.random.seed(42)  # ğŸ¯ Ensures reproducibility (same random values every time)\n",
    "X = np.random.rand(10, 3)  # ğŸ”¢ Create 10 rows, 3 features with random values\n",
    "y = np.random.randint(0, 2, 10)  # ğŸ¯ Generate random binary target values (0 or 1)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `np.random.seed(42)`:  \n",
    "  - Makes sure the **random numbers are the same every time** you run the code.  \n",
    "  - Helps in debugging and ensures **consistent results**. ğŸ¯  \n",
    "- `np.random.rand(10, 3)`:  \n",
    "  - Creates **10 rows** and **3 columns** of **random numbers** between `0` and `1`.  \n",
    "  - These numbers are the **features** of our dataset. ğŸ”¢  \n",
    "- `np.random.randint(0, 2, 10)`:  \n",
    "  - Creates a **target variable** with **10 random values** (either `0` or `1`). ğŸ¯  \n",
    "  - This represents **binary classification**.  \n",
    "\n",
    "ğŸ“Œ **At this point, we have a dataset with**:\n",
    "| Feature1 | Feature2 | Feature3 | Target |\n",
    "|----------|----------|----------|--------|\n",
    "| 0.37     | 0.95     | 0.73     | 1      |\n",
    "| 0.60     | 0.16     | 0.87     | 0      |\n",
    "| 0.06     | 0.87     | 0.60     | 1      |\n",
    "| ...      | ...      | ...      | ...    |\n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸ”€ Step 3: Split Data Into Training & Testing**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- **Splitting Data**:\n",
    "  - `train_test_split()` divides the dataset into **80% training** and **20% testing**. ğŸ”€  \n",
    "- **Parameters**:\n",
    "  - `test_size=0.2`: Means **20% of the data** is for testing.  \n",
    "  - `random_state=42`: Ensures the **split remains the same** every time. ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸ­ Step 4: Define Base Models**\n",
    "```python\n",
    "dt = DecisionTreeClassifier()  # ğŸŒ³ Decision Tree Classifier\n",
    "svm = SVC(probability=True)  # ğŸ“ˆ Support Vector Machine (Enable probability=True for soft voting)\n",
    "nb = GaussianNB()  # ğŸ“Š NaÃ¯ve Bayes Classifier\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- We are defining **three different base models**:\n",
    "  1. **DecisionTreeClassifier()** ğŸŒ³  \n",
    "     - Works by **splitting** the data at different points.  \n",
    "  2. **SVC() (Support Vector Classifier)** ğŸ“ˆ  \n",
    "     - Finds the **best decision boundary** between classes.  \n",
    "     - We set `probability=True` because **soft voting** requires probabilities.  \n",
    "  3. **GaussianNB() (NaÃ¯ve Bayes)** ğŸ“Š  \n",
    "     - Uses **Bayesâ€™ theorem** to classify data based on probability.  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸ—³ï¸ Step 5: Create Voting Classifier**\n",
    "```python\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[(\"Decision Tree\", dt), (\"SVM\", svm), (\"NaÃ¯ve Bayes\", nb)],  \n",
    "    voting=\"soft\"  # ğŸ”¥ Uses predicted probabilities for better performance\n",
    ")\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `VotingClassifier(...)`:  \n",
    "  - Combines different models into **one ensemble model**. ğŸ†  \n",
    "- `estimators=[...]`:  \n",
    "  - Defines the **models we want to use**:\n",
    "    - `\"Decision Tree\"` ğŸŒ³  \n",
    "    - `\"SVM\"` ğŸ“ˆ  \n",
    "    - `\"NaÃ¯ve Bayes\"` ğŸ“Š  \n",
    "- `voting=\"soft\"`:  \n",
    "  - **Soft Voting** means models give **probabilities** for each class instead of just voting.  \n",
    "  - The class with the **highest combined probability wins**. ğŸ”¥  \n",
    "  - **Soft Voting is better** than Hard Voting because it gives more weight to confident predictions.  \n",
    "\n",
    "ğŸ“Œ **Example of Soft Voting**  \n",
    "Letâ€™s say we want to classify a new data point:  \n",
    "| Model | Probability of Class 0 | Probability of Class 1 |\n",
    "|--------|-------------------------|-------------------------|\n",
    "| Decision Tree ğŸŒ³ | 0.4 | 0.6 |\n",
    "| SVM ğŸ“ˆ | 0.2 | 0.8 |\n",
    "| NaÃ¯ve Bayes ğŸ“Š | 0.3 | 0.7 |\n",
    "| **Final Prediction ğŸ—³ï¸** | **0.3 (avg)** | **0.7 (avg) â†’ Class 1 ğŸ¯** |\n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸ‹ï¸ Step 6: Train the Model**\n",
    "```python\n",
    "voting_clf.fit(X_train, y_train)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- **Trains** the entire **ensemble model** using `X_train` and `y_train`. ğŸ‹ï¸  \n",
    "- Each **individual model learns** from the training data.  \n",
    "- The **final VotingClassifier** will use their outputs for predictions. ğŸ¯  \n",
    "\n",
    "---\n",
    "\n",
    "# **ğŸ§ Step 7: Make Predictions**\n",
    "```python\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- Uses the **trained VotingClassifier** to predict results for `X_test`.  \n",
    "- It **combines predictions** from all base models using **soft voting**. ğŸ­  \n",
    "\n",
    "---\n",
    "\n",
    "# **âœ… Step 8: Evaluate Accuracy**\n",
    "```python\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"ğŸ¯ Heterogeneous Ensemble Accuracy:\", accuracy)\n",
    "```\n",
    "### **ğŸ” Whatâ€™s happening here?**\n",
    "- `accuracy_score(y_test, y_pred)`:  \n",
    "  - Compares **actual labels** (`y_test`) with **predicted labels** (`y_pred`).  \n",
    "  - Calculates the **accuracy of the model**. âœ…  \n",
    "- The result is printed as:\n",
    "  ```\n",
    "  ğŸ¯ Heterogeneous Ensemble Accuracy: 0.5\n",
    "  ```\n",
    "  - This means the model predicted **50% of test samples correctly**. ğŸ“Š  \n",
    "\n",
    "---\n",
    "\n",
    "## **ğŸ¯ Final Takeaways**\n",
    "âœ”ï¸ **Bagging uses the same model multiple times** (e.g., 10 Decision Trees ğŸŒ³).  \n",
    "âœ”ï¸ **Voting Classifier allows different models** (e.g., Decision Tree ğŸŒ³, SVM ğŸ“ˆ, NaÃ¯ve Bayes ğŸ“Š).  \n",
    "âœ”ï¸ **Soft Voting considers probabilities**, making it **more accurate** than Hard Voting.  \n",
    "âœ”ï¸ **Each model contributes differently**, making the system more **stable**.  \n",
    "\n",
    "---\n",
    "\n",
    "ğŸš€ğŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
