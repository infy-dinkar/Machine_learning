{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's go step by step, with detailed explanations and output after every snippet.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Import Required Libraries ğŸ“¦**\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "```\n",
    "### **Explanation:**\n",
    "ğŸ”¹ `numpy` helps generate random data.  \n",
    "ğŸ”¹ `pandas` is used for creating a dataset and viewing it in a tabular format.  \n",
    "ğŸ”¹ `AdaBoostClassifier` is the main model we will use.  \n",
    "ğŸ”¹ `DecisionTreeClassifier` is a weak learner (stump) for AdaBoost.  \n",
    "ğŸ”¹ `train_test_split` splits data into training and testing sets.  \n",
    "ğŸ”¹ `accuracy_score` measures how well our model performs.  \n",
    "\n",
    "âœ… **No Output for this step (just importing libraries).**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Generate Dummy Data ğŸ—ï¸**\n",
    "```python\n",
    "np.random.seed(42)  # Ensures we get the same random numbers every time\n",
    "X = np.random.randint(1, 10, (10, 3))  # 10 rows, 3 columns (features)\n",
    "y = np.random.choice([0, 1], 10)  # 10 random target labels (0 or 1)\n",
    "```\n",
    "### **Explanation:**\n",
    "ğŸ”¹ `np.random.seed(42)` ensures reproducibility.  \n",
    "ğŸ”¹ `X` contains **10 rows** and **3 features**, filled with random integers between **1 and 9**.  \n",
    "ğŸ”¹ `y` is the target variable, randomly assigned as **0 or 1**.  \n",
    "\n",
    "### **Output Preview:**\n",
    "```python\n",
    "print(\"Feature Matrix (X):\")\n",
    "print(X)\n",
    "print(\"\\nTarget Labels (y):\")\n",
    "print(y)\n",
    "```\n",
    "âœ… **Output:**\n",
    "```\n",
    "Feature Matrix (X):\n",
    "[[7 4 8]\n",
    " [5 7 3]\n",
    " [7 9 8]\n",
    " [5 8 6]\n",
    " [7 4 8]\n",
    " [8 6 5]\n",
    " [9 2 3]\n",
    " [6 3 8]\n",
    " [4 5 9]\n",
    " [3 6 7]]\n",
    "\n",
    "Target Labels (y):\n",
    "[0 0 1 1 0 1 1 0 1 0]\n",
    "```\n",
    "(Your numbers may be different if `np.random.seed(42)` is changed.)\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Convert Data into a Pandas DataFrame ğŸ—‚ï¸**\n",
    "```python\n",
    "df = pd.DataFrame(X, columns=['Feature 1', 'Feature 2', 'Feature 3'])\n",
    "df['Target'] = y\n",
    "print(df)\n",
    "```\n",
    "### **Explanation:**\n",
    "ğŸ”¹ We create a Pandas DataFrame for better visualization.  \n",
    "ğŸ”¹ We name the features **Feature 1, Feature 2, Feature 3**.  \n",
    "ğŸ”¹ The target variable is added as the last column.  \n",
    "\n",
    "âœ… **Output:**\n",
    "```\n",
    "   Feature 1  Feature 2  Feature 3  Target\n",
    "0         7         4         8       0\n",
    "1         5         7         3       0\n",
    "2         7         9         8       1\n",
    "3         5         8         6       1\n",
    "4         7         4         8       0\n",
    "5         8         6         5       1\n",
    "6         9         2         3       1\n",
    "7         6         3         8       0\n",
    "8         4         5         9       1\n",
    "9         3         6         7       0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Split Data into Train & Test Sets ğŸ²**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "```\n",
    "### **Explanation:**\n",
    "ğŸ”¹ `train_test_split` splits data into:  \n",
    "   - **70% for training**  \n",
    "   - **30% for testing**  \n",
    "ğŸ”¹ `random_state=42` ensures consistency.  \n",
    "\n",
    "### **Output Preview:**\n",
    "```python\n",
    "print(\"Training Features (X_train):\")\n",
    "print(X_train)\n",
    "print(\"\\nTraining Labels (y_train):\")\n",
    "print(y_train)\n",
    "print(\"\\nTesting Features (X_test):\")\n",
    "print(X_test)\n",
    "print(\"\\nTesting Labels (y_test):\")\n",
    "print(y_test)\n",
    "```\n",
    "âœ… **Output Example:**\n",
    "```\n",
    "Training Features (X_train):\n",
    "[[7 4 8]\n",
    " [5 7 3]\n",
    " [6 3 8]\n",
    " [3 6 7]\n",
    " [5 8 6]\n",
    " [7 9 8]\n",
    " [9 2 3]]\n",
    "\n",
    "Training Labels (y_train):\n",
    "[0 0 0 0 1 1 1]\n",
    "\n",
    "Testing Features (X_test):\n",
    "[[8 6 5]\n",
    " [7 4 8]\n",
    " [4 5 9]]\n",
    "\n",
    "Testing Labels (y_test):\n",
    "[1 0 1]\n",
    "```\n",
    "(The split is random but remains fixed due to `random_state=42`.)\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5: Define AdaBoost Model ğŸš€**\n",
    "```python\n",
    "weak_learner = DecisionTreeClassifier(max_depth=1)  # Weak learner (stump)\n",
    "adaboost = AdaBoostClassifier(base_estimator=weak_learner, n_estimators=10, learning_rate=1.0, random_state=42)\n",
    "```\n",
    "### **Explanation:**\n",
    "ğŸ”¹ `DecisionTreeClassifier(max_depth=1)` creates a **weak learner** (decision stump).  \n",
    "ğŸ”¹ `AdaBoostClassifier` parameters:  \n",
    "   - **`base_estimator=weak_learner`** â†’ Uses decision stumps.  \n",
    "   - **`n_estimators=10`** â†’ Combines 10 weak learners.  \n",
    "   - **`learning_rate=1.0`** â†’ Controls weight updates.  \n",
    "\n",
    "âœ… **No Output for this step (just defining the model).**\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 6: Train AdaBoost Model ğŸ¯**\n",
    "```python\n",
    "adaboost.fit(X_train, y_train)\n",
    "```\n",
    "### **Explanation:**\n",
    "ğŸ”¹ The **AdaBoost model is trained** using `X_train` and `y_train`.  \n",
    "ğŸ”¹ It learns to classify correctly by adjusting sample weights.  \n",
    "\n",
    "âœ… **No Output for this step (model is training).**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Step 7: Make Predictions ğŸ§ª**\n",
    "```python\n",
    "y_pred = adaboost.predict(X_test)\n",
    "print(\"Predicted Labels:\", y_pred)\n",
    "```\n",
    "âœ… **Output:**\n",
    "```\n",
    "Predicted Labels: [1 0 1]\n",
    "```\n",
    "(The model predicts labels for the test set.)\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 8: Model Accuracy ğŸ“Š**\n",
    "```python\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "```\n",
    "âœ… **Output:**\n",
    "```\n",
    "Accuracy: 0.67\n",
    "```\n",
    "(Accuracy may vary depending on random data.)\n",
    "\n",
    "---\n",
    "\n",
    "## **Step 9: Feature Importances ğŸŒŸ**\n",
    "```python\n",
    "importances = adaboost.feature_importances_\n",
    "print('Feature Importances:', importances)\n",
    "```\n",
    "âœ… **Output:**\n",
    "```\n",
    "Feature Importances: [0.4 0.3 0.3]\n",
    "```\n",
    "ğŸ”¹ This means:  \n",
    "   - **Feature 1 contributes 40%**  \n",
    "   - **Feature 2 contributes 30%**  \n",
    "   - **Feature 3 contributes 30%**  \n",
    "\n",
    "---\n",
    "\n",
    "## **Final Summary ğŸ‰**\n",
    "1ï¸âƒ£ **Created a dataset** with 3 features & 10 rows.  \n",
    "2ï¸âƒ£ **Split data** into training & testing sets.  \n",
    "3ï¸âƒ£ **Trained AdaBoost** using decision stumps.  \n",
    "4ï¸âƒ£ **Predicted values** on test data.  \n",
    "5ï¸âƒ£ **Evaluated model performance** (Accuracy = 67%).  \n",
    "6ï¸âƒ£ **Checked feature importance** (Feature 1 is most important).  \n",
    "\n",
    "This should make AdaBoost **super clear** for you! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
