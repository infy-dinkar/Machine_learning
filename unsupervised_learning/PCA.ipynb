{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PCA** 🚀😊  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Step 1: Import Required Libraries**\n",
    "```python\n",
    "import numpy as np  # 🔢 For numerical computations\n",
    "import pandas as pd  # 🏷️ Handling datasets\n",
    "import matplotlib.pyplot as plt  # 📊 For visualization\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 🌍 3D plot\n",
    "```\n",
    "### **🔹 Explanation:**\n",
    "- **`numpy`**: Used for numerical operations like matrix manipulations.  \n",
    "- **`pandas`**: Used for handling datasets in a tabular format.  \n",
    "- **`matplotlib.pyplot`**: Used to plot graphs.  \n",
    "- **`mpl_toolkits.mplot3d`**: Required for 3D visualizations.  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Step 2: Define the Dataset**\n",
    "```python\n",
    "data = np.array([\n",
    "    [2.5, 2.4, 3.5],  \n",
    "    [0.5, 0.7, 2.2],  \n",
    "    [2.2, 2.9, 3.1],  \n",
    "    [1.9, 2.2, 3.8],  \n",
    "    [3.1, 3.0, 3.3]  \n",
    "])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Feature 1\", \"Feature 2\", \"Feature 3\"])\n",
    "print(\"Dataset:\\n\", df)\n",
    "```\n",
    "### **🔹 Explanation:**\n",
    "- **`data`**: This is a 5×3 matrix where each row is a data point, and each column is a feature.  \n",
    "- **`pd.DataFrame(data, columns=[...])`**: Converts the numpy array into a Pandas DataFrame with proper column names.  \n",
    "- **`print(df)`**: Displays the dataset in a tabular format.  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Step 3: Standardizing the Data**\n",
    "```python\n",
    "mean_vector = np.mean(data, axis=0)  # Compute mean of each feature\n",
    "standardized_data = data - mean_vector  # Center the data\n",
    "\n",
    "print(\"\\nStandardized Data:\\n\", standardized_data)\n",
    "```\n",
    "### **🔹 Explanation:**\n",
    "- **Why Standardize?** 🤔  \n",
    "  PCA works best when data is **centered** (mean = 0), so we subtract the mean of each column from all values in that column.  \n",
    "\n",
    "- **`np.mean(data, axis=0)`**: Computes the **mean of each column (feature-wise mean).**  \n",
    "- **`data - mean_vector`**: Subtracts the mean from the dataset to center it around **zero mean**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Step 4: Compute Covariance Matrix & Eigenvalues/Vectors**\n",
    "```python\n",
    "cov_matrix = np.cov(standardized_data.T)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "print(\"\\nCovariance Matrix:\\n\", cov_matrix)\n",
    "print(\"\\nEigenvalues:\\n\", eigenvalues)\n",
    "print(\"\\nEigenvectors:\\n\", eigenvectors)\n",
    "```\n",
    "### **🔹 Explanation:**\n",
    "- **What is the covariance matrix?** 📏  \n",
    "  - It represents how two features vary together.  \n",
    "  - Large values mean a **strong relationship** between features.  \n",
    "\n",
    "- **`np.cov(standardized_data.T)`**:  \n",
    "  - **`.T`** transposes the matrix (converts rows to columns).  \n",
    "  - `np.cov()` computes the covariance between features.  \n",
    "\n",
    "- **`np.linalg.eig(cov_matrix)`**:  \n",
    "  - Finds **eigenvalues** (how much variance a direction captures).  \n",
    "  - Finds **eigenvectors** (directions of new principal components).  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Step 5: Project Data onto Principal Component**\n",
    "```python\n",
    "top_eigenvector = eigenvectors[:, np.argmax(eigenvalues)]  # Select the top eigenvector\n",
    "projected_data = standardized_data @ top_eigenvector  # Project onto principal component\n",
    "\n",
    "print(\"\\nProjected Data:\\n\", projected_data)\n",
    "```\n",
    "### **🔹 Explanation:**\n",
    "- **Why do we project data?** 🔄  \n",
    "  - Instead of using 3D data, we **reduce the dimensions** while keeping the most important information.  \n",
    "\n",
    "- **`np.argmax(eigenvalues)`**:  \n",
    "  - Finds the **index of the largest eigenvalue** (most important principal component).  \n",
    "\n",
    "- **`eigenvectors[:, np.argmax(eigenvalues)]`**:  \n",
    "  - Selects the **corresponding eigenvector** (direction of maximum variance).  \n",
    "\n",
    "- **`standardized_data @ top_eigenvector`**:  \n",
    "  - **Matrix multiplication** `@` projects data onto the new principal component.  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Step 6: Visualizing the Data**\n",
    "### **🔹 3D Scatter Plot of Original Data**\n",
    "```python\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(standardized_data[:, 0], standardized_data[:, 1], standardized_data[:, 2], c='b', label='Original Data')\n",
    "\n",
    "ax.set_xlabel(\"Feature 1\")\n",
    "ax.set_ylabel(\"Feature 2\")\n",
    "ax.set_zlabel(\"Feature 3\")\n",
    "ax.set_title(\"Original Data in 3D Space\")\n",
    "ax.legend()\n",
    "plt.show()\n",
    "```\n",
    "### **🔹 Explanation:**\n",
    "- **Creates a 3D plot of the standardized data.**  \n",
    "- **`.scatter()`** plots blue dots representing original data.  \n",
    "- **`ax.set_xlabel()`, `ax.set_ylabel()`, `ax.set_zlabel()`** add labels.  \n",
    "\n",
    "---\n",
    "\n",
    "### **🔹 2D Projection onto Principal Component**\n",
    "```python\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(projected_data, np.zeros_like(projected_data), c='r', label=\"Projected Data\")\n",
    "\n",
    "plt.axhline(0, color='black', linewidth=0.5)  # Draw a horizontal line at 0\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.title(\"Data Projected onto First Principal Component\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "### **🔹 Explanation:**\n",
    "- **Creates a 2D scatter plot where data is projected onto the first principal component.**  \n",
    "- **Data is now reduced from 3D to 1D (X-axis only).**  \n",
    "- **`np.zeros_like(projected_data)`** makes Y values **zero** to align data on a single line.  \n",
    "\n",
    "---\n",
    "\n",
    "## **🎯 Summary of Steps**\n",
    "| **Step** | **What it Does?** |\n",
    "|----------|------------------|\n",
    "| 1️⃣ Import Libraries | Loads necessary Python libraries. |\n",
    "| 2️⃣ Define Dataset | Creates a dataset with 3 features and 5 samples. |\n",
    "| 3️⃣ Standardize Data | Centers data by subtracting mean. |\n",
    "| 4️⃣ Compute Eigenvalues & Eigenvectors | Finds principal components using covariance matrix. |\n",
    "| 5️⃣ Project Data | Reduces 3D data to 1D using top eigenvector. |\n",
    "| 6️⃣ Visualize | Plots original and transformed data. |\n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Key Takeaways**\n",
    "✅ **PCA transforms high-dimensional data into a lower dimension while preserving variance.**  \n",
    "✅ **Eigenvectors define the new axes of transformation.**  \n",
    "✅ **Eigenvalues indicate the importance of each axis (higher = more variance explained).**  \n",
    "✅ **Visualization helps understand how data transforms from 3D to 1D.**  \n",
    "\n",
    "---\n",
    "🚀😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
