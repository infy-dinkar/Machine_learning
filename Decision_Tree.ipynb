{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ ** Guide to Decision Tree Classification & Regression with Pruning in Scikit-Learn** ğŸŒ³\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ **What is a Decision Tree?**  \n",
    "\n",
    "A **Decision Tree** is a **flowchart-like structure** where:  \n",
    "- Each **node** represents a **decision/question** ğŸ§  \n",
    "- Each **branch** represents an **outcome (Yes/No, True/False, etc.)**  \n",
    "- Each **leaf** represents the **final prediction/classification** ğŸ¯  \n",
    "\n",
    "ğŸŒ³ Think of it like **playing \"20 Questions\"** to guess an object. You keep **asking questions** until you reach the **final answer**!\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›  **1ï¸âƒ£ Importing Required Libraries**  \n",
    "Before we start, we need to **import** some essential Python libraries.  \n",
    "\n",
    "```python\n",
    "import numpy as np  # ğŸ”¢ Helps with numerical calculations\n",
    "import pandas as pd  # ğŸ“Š Helps handle datasets like spreadsheets\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_text  # ğŸŒ³ Decision Trees\n",
    "from sklearn.model_selection import train_test_split  # âœ‚ï¸ Splits data into training & testing\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error  # âœ… Model evaluation metrics\n",
    "```\n",
    "\n",
    "### ğŸ§ **Whatâ€™s Happening Here?**\n",
    "âœ… `numpy` (`np`) â†’ Helps in working with **numbers, arrays, and random numbers**  \n",
    "âœ… `pandas` (`pd`) â†’ Used for **handling tabular data (like Excel files)**  \n",
    "âœ… `DecisionTreeClassifier` & `DecisionTreeRegressor` â†’ Implement **Decision Trees**  \n",
    "âœ… `export_text` â†’ Displays a **text-based visualization** of the Decision Tree  \n",
    "âœ… `train_test_split` â†’ Splits data into **Training (80%) and Testing (20%)**  \n",
    "âœ… `accuracy_score` â†’ Measures **classification accuracy**  \n",
    "âœ… `mean_squared_error` â†’ Measures **regression error**  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š **2ï¸âƒ£ Creating a Sample Dataset (Features & Targets)**\n",
    "Since we donâ€™t have a dataset, weâ€™ll **generate random data**.\n",
    "\n",
    "```python\n",
    "np.random.seed(42)  # ğŸ² Ensures we get the same random values every time\n",
    "X = np.random.randint(1, 100, (10, 3))  # ğŸ”¢ Generate a 10x3 matrix (values between 1-100)\n",
    "y_classification = np.random.choice([0, 1], size=10)  # ğŸ¯ Binary classification target (0 or 1)\n",
    "y_regression = np.random.randint(50, 150, 10)  # ğŸ“ˆ Continuous target values (for regression)\n",
    "```\n",
    "\n",
    "### ğŸ§ **Whatâ€™s Happening Here?**\n",
    "âœ… `np.random.seed(42)` â†’ Ensures the same random numbers are generated **each time we run the code**  \n",
    "âœ… `np.random.randint(1, 100, (10, 3))` â†’ Creates **10 rows Ã— 3 features** (random values **between 1-100**)  \n",
    "âœ… `np.random.choice([0, 1], size=10)` â†’ Randomly assigns **0 or 1** as the **classification labels**  \n",
    "âœ… `np.random.randint(50, 150, 10)` â†’ Generates **random continuous values (50-150) for regression**  \n",
    "\n",
    "ğŸ“Œ **Example Data Generated:**  \n",
    "```\n",
    "Features (X)       Classification Target (y_class)   Regression Target (y_reg)\n",
    "[34, 98, 56]  â†’          1                            123\n",
    "[67, 23, 78]  â†’          0                             85\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âœ‚ï¸ **3ï¸âƒ£ Splitting Data into Training & Testing Sets**\n",
    "Before training a model, we **split** the dataset into **training (80%)** and **testing (20%)**.  \n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_classification, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### ğŸ§ **Whatâ€™s Happening Here?**\n",
    "âœ… **`train_test_split(X, y, test_size=0.2, random_state=42)`**  \n",
    "âœ” Splits **80% for training** & **20% for testing**  \n",
    "âœ” Ensures results are **reproducible** with `random_state=42`\n",
    "\n",
    "---\n",
    "\n",
    "## âœ‚ï¸ **4ï¸âƒ£ Pre-Pruning: Controlling Tree Growth (Before Training)**\n",
    "Before training the model, we **limit its complexity** to **prevent overfitting**.\n",
    "\n",
    "```python\n",
    "clf_pruned = DecisionTreeClassifier(\n",
    "    criterion='entropy',  # ğŸ“Š Use Entropy for impurity measurement\n",
    "    max_depth=3,  # â¬‡ï¸ Restrict tree depth to 3 levels\n",
    "    min_samples_split=2,  # ğŸ”¢ Minimum samples needed to split a node\n",
    "    min_samples_leaf=1,  # ğŸƒ Each leaf must have at least 1 sample\n",
    "    min_impurity_decrease=0.01,  # ğŸ›‘ Stops splitting if impurity decrease is too small\n",
    "    random_state=42\n",
    ")\n",
    "clf_pruned.fit(X_train, y_train_class)  # ğŸ¯ Train the pre-pruned classifier\n",
    "y_pred_pruned = clf_pruned.predict(X_test)  # ğŸ” Predict using the pruned model\n",
    "```\n",
    "\n",
    "### ğŸ§ **Whatâ€™s Happening Here?**\n",
    "âœ… **Pre-Pruning prevents overfitting** by **stopping tree growth early**.  \n",
    "- `max_depth=3` â†’ Limits the tree to **3 levels**  \n",
    "- `min_samples_split=2` â†’ Node must have **at least 2 samples** to split  \n",
    "- `min_samples_leaf=1` â†’ Each **leaf node must have at least 1 sample**  \n",
    "- `min_impurity_decrease=0.01` â†’ Stops **unnecessary splits**  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸª“ **5ï¸âƒ£ Post-Pruning (After Training)**\n",
    "Instead of limiting the tree **before training**, we **first train the full tree and prune it later**.\n",
    "\n",
    "```python\n",
    "clf_unpruned = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "clf_unpruned.fit(X_train, y_train_class)\n",
    "\n",
    "# ğŸ” Find best pruning alpha\n",
    "path = clf_unpruned.cost_complexity_pruning_path(X_train, y_train_class)\n",
    "ccp_alphas = path.ccp_alphas  # ğŸ“ˆ List of possible alpha values\n",
    "\n",
    "# ğŸ† Choose a middle-value alpha\n",
    "best_alpha = ccp_alphas[len(ccp_alphas) // 2]\n",
    "clf_post_pruned = DecisionTreeClassifier(criterion='entropy', ccp_alpha=best_alpha, random_state=42)\n",
    "clf_post_pruned.fit(X_train, y_train_class)\n",
    "y_pred_post_pruned = clf_post_pruned.predict(X_test)\n",
    "```\n",
    "\n",
    "### ğŸ§ **Whatâ€™s Happening Here?**\n",
    "âœ… Trains a **full tree first**  \n",
    "âœ… Finds **cost-complexity pruning path**  \n",
    "âœ… Uses **`ccp_alpha`** to **prune unnecessary nodes**  \n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ **6ï¸âƒ£ Decision Tree Regression with Pre-Pruning**\n",
    "```python\n",
    "regressor_pruned = DecisionTreeRegressor(\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_impurity_decrease=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "regressor_pruned.fit(X_train, y_train_reg)\n",
    "y_pred_reg_pruned = regressor_pruned.predict(X_test)\n",
    "```\n",
    "âœ… **Same pre-pruning concepts apply to Regression Trees!**\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **7ï¸âƒ£ Evaluating Model Performance**\n",
    "```python\n",
    "print(\"ğŸ“Š Accuracy (Pre-Pruned):\", accuracy_score(y_test_class, y_pred_pruned))\n",
    "print(\"ğŸ“Š Accuracy (Post-Pruned):\", accuracy_score(y_test_class, y_pred_post_pruned))\n",
    "print(\"ğŸ“‰ Regression MSE (Pre-Pruned):\", mean_squared_error(y_test_reg, y_pred_reg_pruned))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸŒ³ **8ï¸âƒ£ Displaying Decision Tree Structure**\n",
    "```python\n",
    "print(\"\\nğŸŒ³ Pre-Pruned Tree:\")\n",
    "print(export_text(clf_pruned, feature_names=['Feature 1', 'Feature 2', 'Feature 3']))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ **Key Takeaways**\n",
    "âœ… **Pre-Pruning** â†’ Stops tree **early** (limits depth, minimum samples, impurity).  \n",
    "âœ… **Post-Pruning** â†’ Grows **full tree**, then **removes weak branches** (`ccp_alpha`).  \n",
    "\n",
    "Hope this detailed breakdown helps! ğŸš€ Let me know if anything needs more clarity! ğŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
