{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚀 ** Guide to Decision Tree Classification & Regression with Pruning in Scikit-Learn** 🌳\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 **What is a Decision Tree?**  \n",
    "\n",
    "A **Decision Tree** is a **flowchart-like structure** where:  \n",
    "- Each **node** represents a **decision/question** 🧐  \n",
    "- Each **branch** represents an **outcome (Yes/No, True/False, etc.)**  \n",
    "- Each **leaf** represents the **final prediction/classification** 🎯  \n",
    "\n",
    "🌳 Think of it like **playing \"20 Questions\"** to guess an object. You keep **asking questions** until you reach the **final answer**!\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠 **1️⃣ Importing Required Libraries**  \n",
    "Before we start, we need to **import** some essential Python libraries.  \n",
    "\n",
    "```python\n",
    "import numpy as np  # 🔢 Helps with numerical calculations\n",
    "import pandas as pd  # 📊 Helps handle datasets like spreadsheets\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_text  # 🌳 Decision Trees\n",
    "from sklearn.model_selection import train_test_split  # ✂️ Splits data into training & testing\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error  # ✅ Model evaluation metrics\n",
    "```\n",
    "\n",
    "### 🧐 **What’s Happening Here?**\n",
    "✅ `numpy` (`np`) → Helps in working with **numbers, arrays, and random numbers**  \n",
    "✅ `pandas` (`pd`) → Used for **handling tabular data (like Excel files)**  \n",
    "✅ `DecisionTreeClassifier` & `DecisionTreeRegressor` → Implement **Decision Trees**  \n",
    "✅ `export_text` → Displays a **text-based visualization** of the Decision Tree  \n",
    "✅ `train_test_split` → Splits data into **Training (80%) and Testing (20%)**  \n",
    "✅ `accuracy_score` → Measures **classification accuracy**  \n",
    "✅ `mean_squared_error` → Measures **regression error**  \n",
    "\n",
    "---\n",
    "\n",
    "## 📊 **2️⃣ Creating a Sample Dataset (Features & Targets)**\n",
    "Since we don’t have a dataset, we’ll **generate random data**.\n",
    "\n",
    "```python\n",
    "np.random.seed(42)  # 🎲 Ensures we get the same random values every time\n",
    "X = np.random.randint(1, 100, (10, 3))  # 🔢 Generate a 10x3 matrix (values between 1-100)\n",
    "y_classification = np.random.choice([0, 1], size=10)  # 🎯 Binary classification target (0 or 1)\n",
    "y_regression = np.random.randint(50, 150, 10)  # 📈 Continuous target values (for regression)\n",
    "```\n",
    "\n",
    "### 🧐 **What’s Happening Here?**\n",
    "✅ `np.random.seed(42)` → Ensures the same random numbers are generated **each time we run the code**  \n",
    "✅ `np.random.randint(1, 100, (10, 3))` → Creates **10 rows × 3 features** (random values **between 1-100**)  \n",
    "✅ `np.random.choice([0, 1], size=10)` → Randomly assigns **0 or 1** as the **classification labels**  \n",
    "✅ `np.random.randint(50, 150, 10)` → Generates **random continuous values (50-150) for regression**  \n",
    "\n",
    "📌 **Example Data Generated:**  \n",
    "```\n",
    "Features (X)       Classification Target (y_class)   Regression Target (y_reg)\n",
    "[34, 98, 56]  →          1                            123\n",
    "[67, 23, 78]  →          0                             85\n",
    "...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ✂️ **3️⃣ Splitting Data into Training & Testing Sets**\n",
    "Before training a model, we **split** the dataset into **training (80%)** and **testing (20%)**.  \n",
    "\n",
    "```python\n",
    "X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_classification, test_size=0.2, random_state=42)\n",
    "X_train, X_test, y_train_reg, y_test_reg = train_test_split(X, y_regression, test_size=0.2, random_state=42)\n",
    "```\n",
    "\n",
    "### 🧐 **What’s Happening Here?**\n",
    "✅ **`train_test_split(X, y, test_size=0.2, random_state=42)`**  \n",
    "✔ Splits **80% for training** & **20% for testing**  \n",
    "✔ Ensures results are **reproducible** with `random_state=42`\n",
    "\n",
    "---\n",
    "\n",
    "## ✂️ **4️⃣ Pre-Pruning: Controlling Tree Growth (Before Training)**\n",
    "Before training the model, we **limit its complexity** to **prevent overfitting**.\n",
    "\n",
    "```python\n",
    "clf_pruned = DecisionTreeClassifier(\n",
    "    criterion='entropy',  # 📊 Use Entropy for impurity measurement\n",
    "    max_depth=3,  # ⬇️ Restrict tree depth to 3 levels\n",
    "    min_samples_split=2,  # 🔢 Minimum samples needed to split a node\n",
    "    min_samples_leaf=1,  # 🍃 Each leaf must have at least 1 sample\n",
    "    min_impurity_decrease=0.01,  # 🛑 Stops splitting if impurity decrease is too small\n",
    "    random_state=42\n",
    ")\n",
    "clf_pruned.fit(X_train, y_train_class)  # 🎯 Train the pre-pruned classifier\n",
    "y_pred_pruned = clf_pruned.predict(X_test)  # 🔍 Predict using the pruned model\n",
    "```\n",
    "\n",
    "### 🧐 **What’s Happening Here?**\n",
    "✅ **Pre-Pruning prevents overfitting** by **stopping tree growth early**.  \n",
    "- `max_depth=3` → Limits the tree to **3 levels**  \n",
    "- `min_samples_split=2` → Node must have **at least 2 samples** to split  \n",
    "- `min_samples_leaf=1` → Each **leaf node must have at least 1 sample**  \n",
    "- `min_impurity_decrease=0.01` → Stops **unnecessary splits**  \n",
    "\n",
    "---\n",
    "\n",
    "## 🪓 **5️⃣ Post-Pruning (After Training)**\n",
    "Instead of limiting the tree **before training**, we **first train the full tree and prune it later**.\n",
    "\n",
    "```python\n",
    "clf_unpruned = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
    "clf_unpruned.fit(X_train, y_train_class)\n",
    "\n",
    "# 🔍 Find best pruning alpha\n",
    "path = clf_unpruned.cost_complexity_pruning_path(X_train, y_train_class)\n",
    "ccp_alphas = path.ccp_alphas  # 📈 List of possible alpha values\n",
    "\n",
    "# 🏆 Choose a middle-value alpha\n",
    "best_alpha = ccp_alphas[len(ccp_alphas) // 2]\n",
    "clf_post_pruned = DecisionTreeClassifier(criterion='entropy', ccp_alpha=best_alpha, random_state=42)\n",
    "clf_post_pruned.fit(X_train, y_train_class)\n",
    "y_pred_post_pruned = clf_post_pruned.predict(X_test)\n",
    "```\n",
    "\n",
    "### 🧐 **What’s Happening Here?**\n",
    "✅ Trains a **full tree first**  \n",
    "✅ Finds **cost-complexity pruning path**  \n",
    "✅ Uses **`ccp_alpha`** to **prune unnecessary nodes**  \n",
    "\n",
    "---\n",
    "\n",
    "## 📈 **6️⃣ Decision Tree Regression with Pre-Pruning**\n",
    "```python\n",
    "regressor_pruned = DecisionTreeRegressor(\n",
    "    max_depth=3,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_impurity_decrease=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "regressor_pruned.fit(X_train, y_train_reg)\n",
    "y_pred_reg_pruned = regressor_pruned.predict(X_test)\n",
    "```\n",
    "✅ **Same pre-pruning concepts apply to Regression Trees!**\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **7️⃣ Evaluating Model Performance**\n",
    "```python\n",
    "print(\"📊 Accuracy (Pre-Pruned):\", accuracy_score(y_test_class, y_pred_pruned))\n",
    "print(\"📊 Accuracy (Post-Pruned):\", accuracy_score(y_test_class, y_pred_post_pruned))\n",
    "print(\"📉 Regression MSE (Pre-Pruned):\", mean_squared_error(y_test_reg, y_pred_reg_pruned))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🌳 **8️⃣ Displaying Decision Tree Structure**\n",
    "```python\n",
    "print(\"\\n🌳 Pre-Pruned Tree:\")\n",
    "print(export_text(clf_pruned, feature_names=['Feature 1', 'Feature 2', 'Feature 3']))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 🎯 **Key Takeaways**\n",
    "✅ **Pre-Pruning** → Stops tree **early** (limits depth, minimum samples, impurity).  \n",
    "✅ **Post-Pruning** → Grows **full tree**, then **removes weak branches** (`ccp_alpha`).  \n",
    "\n",
    "Hope this detailed breakdown helps! 🚀 Let me know if anything needs more clarity! 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
