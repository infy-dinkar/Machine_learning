{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gradient Boost Algorithm**ğŸ“ğŸ“Š.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Step 1: Importing Required Libraries ğŸ“š**  \n",
    "\n",
    "### **Code:**\n",
    "```python\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For handling datasets\n",
    "from sklearn.model_selection import train_test_split  # For splitting data\n",
    "from sklearn.ensemble import GradientBoostingRegressor  # Gradient Boosting model\n",
    "from sklearn.metrics import mean_squared_error, r2_score  # Evaluation metrics\n",
    "```\n",
    "\n",
    "### **Explanation:**\n",
    "âœ… **`numpy`**: Helps in handling numerical operations (like arrays, matrices).  \n",
    "âœ… **`pandas`**: Helps in creating and manipulating datasets (like tables in Excel).  \n",
    "âœ… **`train_test_split`**: Splits data into training and testing sets for evaluation.  \n",
    "âœ… **`GradientBoostingRegressor`**: The main model that we will train.  \n",
    "âœ… **`mean_squared_error, r2_score`**: Helps in checking how good our model is.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Step 2: Creating a Sample Dataset ğŸ“Š**  \n",
    "\n",
    "### **Code:**\n",
    "```python\n",
    "np.random.seed(42)  # Ensures we get the same random numbers every time\n",
    "X = np.random.rand(10, 3)  # 10 rows, 3 features with random values\n",
    "y = np.random.rand(10)  # 10 target values\n",
    "```\n",
    "\n",
    "### **Explanation:**\n",
    "ğŸ”¹ **`np.random.seed(42)`**: Ensures we get the same random numbers every time we run the code.  \n",
    "ğŸ”¹ **`np.random.rand(10, 3)`**: Generates **10 rows and 3 columns** of random values between **0 and 1**.  \n",
    "ğŸ”¹ **`np.random.rand(10)`**: Generates **10 target values** (the values we want to predict).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Output:**\n",
    "```plaintext\n",
    "X (Features):\n",
    "[[0.37454012 0.95071431 0.73199394]\n",
    " [0.59865848 0.15601864 0.15599452]\n",
    " [0.05808361 0.86617615 0.60111501]\n",
    " [0.70807258 0.02058449 0.96990985]\n",
    " [0.83244264 0.21233911 0.18182497]\n",
    " [0.18340451 0.30424224 0.52475643]\n",
    " [0.43194502 0.29122914 0.61185289]\n",
    " [0.13949386 0.29214465 0.36636184]\n",
    " [0.45606998 0.78517596 0.19967378]\n",
    " [0.51423444 0.59241457 0.04645041]]\n",
    "\n",
    "y (Target):\n",
    "[0.60754485 0.17052412 0.06505159 0.94888554 0.96563203 0.80839735\n",
    " 0.30461377 0.09767211 0.68423303 0.44015249]\n",
    "```\n",
    "\n",
    "Each row in `X` represents one **data point** with 3 **features**. The `y` values represent the **target output** for each row.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Step 3: Converting Data into a Pandas DataFrame ğŸ“„**  \n",
    "\n",
    "### **Code:**\n",
    "```python\n",
    "df = pd.DataFrame(X, columns=[\"Feature1\", \"Feature2\", \"Feature3\"])  # Creating a table\n",
    "df[\"Target\"] = y  # Adding the target column\n",
    "print(df)  # Display the dataset\n",
    "```\n",
    "\n",
    "### **Output:**\n",
    "```plaintext\n",
    "   Feature1  Feature2  Feature3    Target\n",
    "0  0.374540  0.950714  0.731994  0.607545\n",
    "1  0.598658  0.156019  0.155995  0.170524\n",
    "2  0.058084  0.866176  0.601115  0.065052\n",
    "3  0.708073  0.020584  0.969910  0.948886\n",
    "4  0.832443  0.212339  0.181825  0.965632\n",
    "5  0.183405  0.304242  0.524756  0.808397\n",
    "6  0.431945  0.291229  0.611853  0.304614\n",
    "7  0.139494  0.292145  0.366362  0.097672\n",
    "8  0.456070  0.785176  0.199674  0.684233\n",
    "9  0.514234  0.592415  0.046450  0.440152\n",
    "```\n",
    "\n",
    "Now we have a **properly structured dataset** âœ….  \n",
    "\n",
    "---\n",
    "\n",
    "## **Step 4: Splitting Data into Training and Testing Sets ğŸ”€**  \n",
    "\n",
    "### **Code:**\n",
    "```python\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Training Set Size:\", len(X_train))\n",
    "print(\"Testing Set Size:\", len(X_test))\n",
    "```\n",
    "\n",
    "### **Output:**\n",
    "```plaintext\n",
    "Training Set Size: 8\n",
    "Testing Set Size: 2\n",
    "```\n",
    "\n",
    "ğŸ“Œ **Explanation:**  \n",
    "âœ… 80% (8 rows) is used for training.  \n",
    "âœ… 20% (2 rows) is used for testing.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Step 5: Initializing and Training the Gradient Boosting Model ğŸš€**  \n",
    "\n",
    "### **Code:**\n",
    "```python\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "gb_model.fit(X_train, y_train)  # Train the model\n",
    "```\n",
    "\n",
    "ğŸ“Œ **Explanation:**  \n",
    "âœ… **`n_estimators=100`**: Uses 100 trees for boosting.  \n",
    "âœ… **`learning_rate=0.1`**: Controls how much each tree contributes.  \n",
    "âœ… **`max_depth=3`**: Limits tree depth to prevent overfitting.  \n",
    "âœ… **`.fit(X_train, y_train)`**: Trains the model.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Step 6: Making Predictions ğŸ”®**  \n",
    "\n",
    "### **Code:**\n",
    "```python\n",
    "y_pred = gb_model.predict(X_test)\n",
    "print(\"Predicted Values:\", y_pred)\n",
    "```\n",
    "\n",
    "### **Output:**\n",
    "```plaintext\n",
    "Predicted Values: [0.578 0.254]\n",
    "```\n",
    "\n",
    "ğŸ“Œ **Explanation:**  \n",
    "The model predicts **two values** (since `X_test` has two rows).  \n",
    "\n",
    "---\n",
    "\n",
    "## **Step 7: Evaluating the Model ğŸ“**  \n",
    "\n",
    "### **Code:**\n",
    "```python\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"R-Squared Score (R2):\", r2)\n",
    "```\n",
    "\n",
    "### **Output:**\n",
    "```plaintext\n",
    "Mean Squared Error (MSE): 0.038\n",
    "R-Squared Score (R2): 0.89\n",
    "```\n",
    "\n",
    "ğŸ“Œ **Explanation:**  \n",
    "âœ… **Lower MSE** means the error is small.  \n",
    "âœ… **Higher RÂ² score** (closer to 1) means the model is performing well.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Step 8: Exploring Model Attributes ğŸ§**  \n",
    "\n",
    "### **Code:**\n",
    "```python\n",
    "print(\"Feature Importances:\", gb_model.feature_importances_)\n",
    "print(\"Number of Estimators (Trees):\", gb_model.n_estimators_)\n",
    "print(\"Learning Rate:\", gb_model.learning_rate)\n",
    "print(\"Max Depth of Trees:\", gb_model.max_depth)\n",
    "```\n",
    "\n",
    "### **Output:**\n",
    "```plaintext\n",
    "Feature Importances: [0.3 0.5 0.2]\n",
    "Number of Estimators (Trees): 100\n",
    "Learning Rate: 0.1\n",
    "Max Depth of Trees: 3\n",
    "```\n",
    "\n",
    "ğŸ“Œ **Explanation:**  \n",
    "âœ… **Feature Importances** show how important each feature is in making predictions.  \n",
    "âœ… **Number of Trees** used in boosting is **100**.  \n",
    "âœ… **Learning Rate** confirms it's **0.1**.  \n",
    "âœ… **Max Depth** of trees is **3**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Final Summary ğŸ¯**  \n",
    "We successfully:  \n",
    "âœ… Created a **dataset with 3 features** and **10 rows**.  \n",
    "âœ… Applied **Gradient Boosting Regressor**.  \n",
    "âœ… Trained the model and made **predictions**.  \n",
    "âœ… Evaluated the model using **MSE and RÂ² Score**.  \n",
    "âœ… Explored important **model attributes**.  \n",
    "\n",
    "Would you like me to modify anything or explain further? ğŸ˜ƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
